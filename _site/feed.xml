<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-23T15:52:58-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Justin Gong</title><subtitle>Data Science Portfolio</subtitle><author><name>Justin Gong</name></author><entry><title type="html">Practicum - NHANES</title><link href="http://localhost:4000/nhanes/" rel="alternate" type="text/html" title="Practicum - NHANES" /><published>2019-05-01T00:00:00-07:00</published><updated>2019-05-01T00:00:00-07:00</updated><id>http://localhost:4000/nhanes</id><content type="html" xml:base="http://localhost:4000/nhanes/">&lt;p&gt;Details for this project are available on &lt;a href=&quot;https://github.com/jjgong7/NHANES-Practicum&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective:&lt;/h2&gt;

&lt;p&gt;The objective of the practicum is to identify the prevalence and risk factors of hospital utilization and major diseases over time using publicly reported qualitative and quantitative data. To help answer this objective, I will explore five main questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What factors predict the likelihood of a patient utilizing hospital resources?&lt;/li&gt;
  &lt;li&gt;Are there different risk factors associated with different clinical categories or clusters of patients?&lt;/li&gt;
  &lt;li&gt;How can the prevalence of major diseases and risk factors be depicted visually to policy makers?&lt;/li&gt;
  &lt;li&gt;The prevalence of certain diseases has been thought to be increasing over time. As disease prevalence increases, has the use of hospital resources also increased? Additionally, what socio-economic factors are associated with increases in disease prevalence?&lt;/li&gt;
  &lt;li&gt;Has the prevalence of undiagnosed conditions increased in rural populations over time, compared to non-rural populations?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This project is split into six parts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Part one:
    &lt;ul&gt;
      &lt;li&gt;Identify the risk factors and prevalence of hospital utilization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parts two to five:
    &lt;ul&gt;
      &lt;li&gt;Identify the risk factors and prevalence of four major diseases – Heart Disease, Cancer, Chronic Lung Disease and Diabetes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Part six:
    &lt;ul&gt;
      &lt;li&gt;Identify the prevalence of undiagnosed Diabetes, where BMI &amp;gt; 30.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;introduction-to-nhanes&quot;&gt;Introduction to NHANES&lt;/h2&gt;
&lt;p&gt;“The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation.&lt;/p&gt;

&lt;p&gt;The NHANES program began in the early 1960s and has been conducted as a series of surveys focusing on different population groups or health topics. In 1999, the survey became a continuous program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs. The survey examines a nationally representative sample of about 5,000 persons each year. These persons are located in counties across the country, 15 of which are visited each year.&lt;/p&gt;

&lt;p&gt;The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel. Findings from this survey will be used to determine the prevalence of major diseases and risk factors for diseases.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The data is presented in 2 year groups from 1999 to 2016 (1999-2000, 2001-2002, etc.). The 18-year data consists of 1,222 XPT files for about ~3.2 GB. Each 2-year span includes information about demographic, dietary, examination, laboratory, and questionnaire data. An individual in each 2-year span has a unique id labeled ‘SEQN’.&lt;/p&gt;

&lt;h2 id=&quot;data-cleaning-and-processing&quot;&gt;Data Cleaning and Processing&lt;/h2&gt;
&lt;p&gt;The following data files are used for this project:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Demographics Data&lt;/li&gt;
  &lt;li&gt;Dietary Data: Dietary Interview for First Day&lt;/li&gt;
  &lt;li&gt;Examination Data: Blood Pressure Measurement, Total Cholesterol, LDL – Cholesterol, HDL – Cholesterol, Body Measures&lt;/li&gt;
  &lt;li&gt;Questionnaire Data: Alcohol Use, Blood Pressure, Diabetes, Health Insurance, Hospital Utilization, Medical Conditions, Physical Activity, Respiratory Health, Smoking, Household Smoking, Weight History&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;a-data-cleaning&quot;&gt;a. Data Cleaning&lt;/h3&gt;
&lt;p&gt;Data cleaning and processing are performed in Python using Jupyter notebooks. The files were converted from XPT to CSV format into the appropriate folders. Features were renamed and values were mapped to maintain consistency across the years. Records with values that were missing, “Refused”, or “Don’t Know” were removed since most of it consisted of less than 10% of the data. The cleaned data is uploaded to a local NoSQL (MongoDB) database.&lt;/p&gt;

&lt;h3 id=&quot;b-data-processing&quot;&gt;b. Data Processing&lt;/h3&gt;
&lt;p&gt;For hospital utilization and major disease analysis, an inner join was performed on the IDs for the appropriate data files. One-hot encoding was performed on categorical variables since Scikit-learn’s Random Forest requires it. Labels for each analysis part was remapped as 1 indicating the minority and 0 indicating the majority class. This is reversed from the default in NHANES (0-Yes, 1-No). The collections for analysis are uploaded to a local uploaded to a local NoSQL (MongoDB) database as well.&lt;/p&gt;

&lt;h2 id=&quot;algorithms-used&quot;&gt;Algorithms Used&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Random Forest:&lt;/strong&gt;
This is a supervised learning ensemble method that can be used for classification and regression. I will be using it for classification. Random forests are composed of multiple decisions trees with randomly selected subsets of features for each tree. It produces an output of the majority vote of the classes. Random forests use bagging to train each decision tree, which means it selects a random sample of about 2/3 of the training data with replacement and then uses the remaining 1/3 data, the Out-Of-Bag, to produce a generalization error. Bagging or bootstrap aggregating performs better since it’s able to decrease the variance of the model, without increasing bias. The features are split via Gini importance, which is computed as the (normalized) total reduction of the criterion brought by that feature.&lt;/p&gt;

&lt;p&gt;The benefits of random forests are that it corrects for decision trees’ overfitting to the training set, features don’t have to be scaled, it can handle missing values, and it gives good indication of feature importance. Scikit-learn in Python is used for analysis and to get the feature importance.&lt;/p&gt;

&lt;p&gt;The following hyperparameters are tuned for the Random Forest algorithm via GridSearchCV:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;n_estimators: The number of trees in the forest&lt;/li&gt;
  &lt;li&gt;max_features: The number of features to consider at each split&lt;/li&gt;
  &lt;li&gt;max_depth: Maximum depth of the tree&lt;/li&gt;
  &lt;li&gt;criterion: Function to measure split (Gini)&lt;/li&gt;
  &lt;li&gt;class_weight: Weights associated with the classes (Imbalanced class)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;XGBoost:&lt;/strong&gt; 
This is a supervised algorithm that has been popular among Kaggle competitions. XGBoost stands for eXtreme Gradient Boosting and implements a gradient boosted decision tree algorithm for classification or regression. Gradient Boosting uses weighting to convert weak learners (ones that do slightly better than chance) into a strong learner. Errors in the previous trees are thus reduced. Boosting also makes use of trees with fewer splits to prevent overfitting.&lt;/p&gt;

&lt;p&gt;The benefits also include model performance, execution speed, handle missing values, and the ability to provide feature importance. Scikit-learn and XGBoost Classifier in Python is used for analysis and to get the feature importance.&lt;/p&gt;

&lt;p&gt;The following hyperparameters are tuned for the XGBoost algorithm via GridSearchCV:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;n_estimators: The number of trees in the forest&lt;/li&gt;
  &lt;li&gt;max_depth: Maximum depth of tree&lt;/li&gt;
  &lt;li&gt;subsample: Subsample ratio of the training instances (prevent overfitting)&lt;/li&gt;
  &lt;li&gt;colsample_bytree: Subsampling ratio of columns when constructing each tree&lt;/li&gt;
  &lt;li&gt;scale_pos_weight: Control the balance of positive and negative weights, useful for unbalanced classes.&lt;/li&gt;
  &lt;li&gt;learning_rate: Step size shrinkage used in updates to prevent overfitting.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;
&lt;p&gt;The analysis for parts one through five have very similar procedures:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Data is retrieved from the local MongoDB.&lt;/li&gt;
  &lt;li&gt;Exploratory Data Analysis is performed.
    &lt;ul&gt;
      &lt;li&gt;A count plot is created for categorical features to see the distribution of categories and a box plot for numerical data to identify quartiles, variability, and outliers. Class distribution is plotted overall and across the 18 years to identify any trends.&lt;/li&gt;
      &lt;li&gt;A Pearson’s Coefficient Correlation plot is created for numerical data - primarily dietary data, cholesterol, and blood pressure, to show the correlation of variables and remove any highly correlated variables (measuring the same factor in different scale)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data is split into training and test sets 80% and 20%, respectively, and stratified across years.&lt;/li&gt;
  &lt;li&gt;Data is Upsampled (SMOTE) and Downsampled (Tomek Links). Since we are working with health data, the class labels are highly imbalanced and up and down sampling is performed to see if models can be better fitted.
    &lt;ul&gt;
      &lt;li&gt;SMOTE, Synthetic Minority Over-sampling Technique, is a common up sampling technique that considers its k-nearest neighbors and the current point and creates a synthetic data point using features between them.&lt;/li&gt;
      &lt;li&gt;Tomek links works by removing overlap between classes. Majority class links are removed until all minimally distanced nearest pairs are of the same class. This puts kind of a buffer, or border, between the classes. This creates less data overall.&lt;/li&gt;
      &lt;li&gt;Imbalanced-learn in Python is used for the sampling.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unnecessary features are removed such as MEC and Dietary 18 Year weights along with years.&lt;/li&gt;
  &lt;li&gt;A filtering method for feature selection is performed.
    &lt;ul&gt;
      &lt;li&gt;Chi-2 analysis is performed to get a sense of the important features (Risk Factors). However, in this analysis, dietary information with high values are weighted heavily.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Random Forest Analysis:
    &lt;ul&gt;
      &lt;li&gt;The regular, Upsampled, and Downsampled data are all used.&lt;/li&gt;
      &lt;li&gt;A parameter grid is defined for GridSearchCV for hyperparameter tuning of the hyperparameters stated above.&lt;/li&gt;
      &lt;li&gt;The best hyperparameters are selected for the Regular, Upsampled, and Downsampled datasets.&lt;/li&gt;
      &lt;li&gt;Random Forest Classifiers are set using the best parameters and fit using the data.&lt;/li&gt;
      &lt;li&gt;Feature importance plot is generated&lt;/li&gt;
      &lt;li&gt;Testing Metrics: Test Data is used for Evaluation
        &lt;ul&gt;
          &lt;li&gt;Confusion Matrix&lt;/li&gt;
          &lt;li&gt;Precision and Recall&lt;/li&gt;
          &lt;li&gt;F1-Score (Main evaluation metric used)
            &lt;ul&gt;
              &lt;li&gt;The F1 score is the harmonic mean of precision and recall: 2TP/(2TP+FP+FN).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;ROC AUC Scores&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Since classifiers are classifying based on probability over a certain amount (0.5 default), different probabilities are iterated over to see if it would produce better f1-scores. A box plot is generated for each cutoff value. Most of the time, the optimal cutoff is still 0.5. The adjusted cutoff value is then compared to the 0.5 cutoff.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;XGBoost Analysis:
    &lt;ul&gt;
      &lt;li&gt;The regular, Upsampled, and Downsampled data are all used.&lt;/li&gt;
      &lt;li&gt;A parameter grid is defined for GridSearchCV for hyperparameter tuning of the hyperparameters stated above.&lt;/li&gt;
      &lt;li&gt;The best hyperparameters are selected for the Regular, Upsampled, and Downsampled datasets.&lt;/li&gt;
      &lt;li&gt;XGBoost Classifiers are set using the best parameters and fit using the data.&lt;/li&gt;
      &lt;li&gt;Testing Metrics: Test Data is used for Evaluation
        &lt;ul&gt;
          &lt;li&gt;Confusion Matrix&lt;/li&gt;
          &lt;li&gt;Precision and Recall&lt;/li&gt;
          &lt;li&gt;F1-Score (Main evaluation metric used)
            &lt;ul&gt;
              &lt;li&gt;The F1 score is the harmonic mean of precision and recall: 2TP/(2TP+FP+FN).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;ROC AUC Scores&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Feature importance plot is generated. XGBoost has two different feature importance measurements
        &lt;ul&gt;
          &lt;li&gt;One is based on scoring feature of f1-score and another feature importance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;XGBoost is compared to Random Forest Analysis&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Identifying the Risk Factors
    &lt;ul&gt;
      &lt;li&gt;Risk factors are identified by feature importance from Random Forest and XGBoost and also feature selection using Chi-2 analysis. The top 10 feature importance are extracted from the Chi-2 analysis, (Regular, Upsample, Downsample) from Random Forest feature importance, XGBoost F1-score importance, and XGBoost feature importance. The top ten for each are weighted 0.025, .15, .10, and 0.075, respectively, to get the overall feature importance for identifying risk factors.
        &lt;ul&gt;
          &lt;li&gt;0.025 x [Chi-2] + .15 [Reg RF + Up RF + Down RF] + .10 [Reg XGB F1 + Up XGB F1 + Down XGB F1] + 0.075 [Reg XGB FI + Up XGB FI + Down XGB FI]&lt;/li&gt;
          &lt;li&gt;Weights add up to 1: 0.025 + 3 x .15 + 3 x .10 + 3 x 0.075 = 1&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;data-visualization-prevalence-of-major-diseases-and-risk-factors-over-time&quot;&gt;Data Visualization: Prevalence of Major Diseases and Risk Factors over time&lt;/h2&gt;
&lt;p&gt;Data visualization (ggplot2) was performed in R. The survey library was used to take into account the complex survey design of NHANES. Error box plots with confidence intervals were generated to show the prevalence of major diseases based on risk factors and prevalence of major diseases over time.&lt;/p&gt;

&lt;h3 id=&quot;part-1-hospital-utilization&quot;&gt;Part 1: Hospital Utilization&lt;/h3&gt;
&lt;p&gt;To predict the likelihood of hospital utilization, the label ‘HUQ050’ is used in the HUQ – Hospital Utilization file. This label indicates the number of times an individual received healthcare over the past year.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initially five classes from 0 (No Hospital Utilization) to 5 (Thirteen or more times received healthcare).&lt;/li&gt;
  &lt;li&gt;There were too many classes, so I mapped the classes to 0 [Received Healthcare (Majority Class)] and 1 [Didn’t Receive Healthcare (Minority Class)].&lt;/li&gt;
  &lt;li&gt;There are 64,583 records with 52 features and ~14% are in the minority class (Didn’t Receive Healthcare).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Random Forest:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class_weight hyperparameter was heavily weighted towards the minority class {0: 0.2, 1: 0.8} for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 15, since it has more balanced data and so class_weight = ‘balanced’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (No Hospital Utilization)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.50&lt;/li&gt;
      &lt;li&gt;Precision: 0.45
        &lt;ul&gt;
          &lt;li&gt;0.45 selected items are relevant&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Recall: 0.55
        &lt;ul&gt;
          &lt;li&gt;0.55 relevant items are selected&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output of feature importance for Random Forest (Left) and XGBoost (Right):&lt;br /&gt;
&lt;!-- &lt;img src=&quot;../images/practicum/P1_HU_RFXG.png&quot; alt=&quot;P1_HU_RFXG&quot;&gt; --&gt;
&lt;a href=&quot;../images/practicum/P1_HU_RFXG.png&quot;&gt; &lt;img src=&quot;../images/practicum/P1_HU_RFXG.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;XGBoost:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scale_pos_weight hyperparameter was weighted towards the minority class by 3 for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 4 compared to 2, since it has more balanced data and so scale_pos_weight was also less at 2.&lt;/li&gt;
  &lt;li&gt;The max depth is much less than random forest.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (No Hospital Utilization)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.51&lt;/li&gt;
      &lt;li&gt;Precision: 0.47&lt;/li&gt;
      &lt;li&gt;Recall: 0.55&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;XGBoost versus Random Forest:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;XGBoost improved F1 score by 0.01 for both classes, and improved precision by 0.02 for the minority class&lt;/li&gt;
  &lt;li&gt;XGBoost outperforms random forest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The following combines the weighted feature importance of the algorithms to identify the risk factors of hospital utilization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P1_HU_RF.png&quot; alt=&quot;P1_HU_RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This shows that some of the important risk factors for hospital utilization are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;HUQ030_2.0– Routine place to go for healthcare (0 – Yes, 1 - No)&lt;/li&gt;
  &lt;li&gt;HID010_2.0 – Covered by health insurance (0 – Yes, 1 - No)&lt;/li&gt;
  &lt;li&gt;RIAGENDR_2.0 – Male or Female (1 – Female)&lt;/li&gt;
  &lt;li&gt;RIDAGEYR – Age at screening&lt;/li&gt;
  &lt;li&gt;DMDCITZN_2.0 – Not a citizen of the U.S. (1 – Not a Citizen)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Prevalence of Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The prevalence of Hospital Utiliztaion and its risk factors are depicted visually using ggplot2 in R.&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P1_HU_V1.png&quot; alt=&quot;P1_HU_V1&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P1_HU_V1.png&quot;&gt; &lt;img src=&quot;../images/practicum/P1_HU_V1.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Most people do have healthcare, are covered by health insurance, and are U.S. citizens so the proportions are higher, but the minority does seem relatively high for each.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P1_HU_V2.png&quot; alt=&quot;P1_HU_V2&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P1_HU_V2.png&quot;&gt; &lt;img src=&quot;../images/practicum/P1_HU_V2.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Females tend to go to the hospital more so than males.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P1_HU_V3.png&quot; alt=&quot;P1_HU_V3&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P1_HU_V3.png&quot;&gt; &lt;img src=&quot;../images/practicum/P1_HU_V3.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As we further break down the age grouping, it shows that children 0 - 12 go to the hospital a lot and this makes sense. Young children are likely to get sick from school or go to the hospital for vaccines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Hospital Utilization over Time:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P1_HU_Time.png&quot; alt=&quot;P1_HU_Time&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From 1999, there does seem to be a very slight increase in hospital utilization over time. The lowest was in 1999-2000 and the highest was most recently in 2013-2014.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;major-diseases-part-2-to-part-5&quot;&gt;Major Diseases: Part 2 to Part 5&lt;/h2&gt;
&lt;p&gt;Question 2: Are there different risk factors associated with different clinical categories or clusters of patients?&lt;br /&gt;
Question 3: How can the prevalence of major diseases and risk factors be depicted visually to policy makers?&lt;br /&gt;
Question 4: The prevalence of certain diseases has been thought to be increasing over time. As disease prevalence increases, has the use of hospital resources also increased? Additionally, what socio-economic factors are associated with increases in disease prevalence?&lt;/p&gt;

&lt;h3 id=&quot;part-2-heart-disease&quot;&gt;Part 2: Heart Disease&lt;/h3&gt;
&lt;p&gt;To see different risk factors associated with heart disease, the label ‘MCQ160C’ is used in the MCQ – Medical Questionnaire file. This label indicates if “You were ever told you had coronary heart disease”.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mapped the classes to 0 – No Heart Disease (Majority) and 1 – Heart Disease (Minority).&lt;/li&gt;
  &lt;li&gt;There are 14,784 records with 65 features and ~4% are in the minority class (Heart Disease).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Random Forest:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class_weight hyperparameter was heavily weighted towards the minority class for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 15, since it has more data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Heart Disease)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.30&lt;/li&gt;
      &lt;li&gt;Precision: 0.20&lt;/li&gt;
      &lt;li&gt;Recall: 0.59&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P2_HD_RFXG.png&quot; alt=&quot;P2_HD_RFXG&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P2_HD_RFXG.png&quot;&gt; &lt;img src=&quot;../images/practicum/P2_HD_RFXG.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;XGBoost:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scale_pos_weight hyperparameter was weighted towards the minority class with a value of 10 for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 6 compared to 3 since it has more data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Heart Disease)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.32&lt;/li&gt;
      &lt;li&gt;Precision: 0.22&lt;/li&gt;
      &lt;li&gt;Recall: 0.61&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;XGBoost versus Random Forest:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The max depth of the trees is much less than random forest at 6 vs 15 for the upsample.&lt;/li&gt;
  &lt;li&gt;XGBoost improved f1-score by .02 for the minority class by improving precision and recall by 0.02 for the minority class.&lt;/li&gt;
  &lt;li&gt;XGBoost performs better!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The following combines the weighted feature importance of the algorithms to identify the risk factors of heart disease.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P2_HD_RF.png&quot; alt=&quot;P1_HU_RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This shows that some of the important risk factors for Heart Disease are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;RIDAGEYR – Age at Screening&lt;/li&gt;
  &lt;li&gt;BPQ020_2.0 – Ever told you had high blood pressure (0 – Yes, 1 – No)&lt;/li&gt;
  &lt;li&gt;RIAGENDR_2.0 – Male or Female (1 – Female)&lt;/li&gt;
  &lt;li&gt;LBXTC – Total cholesterol (mg/dL)&lt;/li&gt;
  &lt;li&gt;LBDLDL – LDL-cholesterol (mg/dL)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some of the features like HUQ010 (General health Condition) and HUQ050 (# Time Received Healthcare) are a result of the disease, so it’s not considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevalence of Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The prevalence of Heart Disease and its risk factors are depicted visually using ggplot2 in R.&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P2_HD_V1.png&quot; alt=&quot;P2_HD_V1&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P2_HD_V1.png&quot;&gt; &lt;img src=&quot;../images/practicum/P2_HD_V1.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Most people who identified as having heart disease did have desirable total cholesterol and LDL cholesterol levels. However, high total cholesterol and LDL levels are still a good indicator of heart disease.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P2_HD_V2.png&quot; alt=&quot;P2_HD_V2&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P2_HD_V2.png&quot;&gt; &lt;img src=&quot;../images/practicum/P2_HD_V2.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is more prevalence of heart disease in males than females. Those who do have high blood pressure also make up a large proportion of people who have heart disease.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P2_HD_V3.png&quot; alt=&quot;P2_HD_V3&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P2_HD_V3.png&quot;&gt; &lt;img src=&quot;../images/practicum/P2_HD_V3.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Those most affected by heart disease are within the 64-85 age range group. As we break it down further, 66-75 year old’s within that group are most affected.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Heart Disease and Hospital Utilization over Time:&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P2_HD_Time.png&quot; alt=&quot;P2_HD_Time&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P2_HD_Time.png&quot;&gt; &lt;img src=&quot;../images/practicum/P2_HD_Time.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Heart disease doesn’t seem to be increasing over time.&lt;/li&gt;
  &lt;li&gt;Hospital utilization linked with those who responded to the MEC questionnaire seem to indicate an increase.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ################################################################## --&gt;

&lt;h3 id=&quot;part-3-cancer&quot;&gt;Part 3: Cancer&lt;/h3&gt;
&lt;p&gt;To see different risk factors associated with cancer, the label ‘MCQ220’ is used in the MCQ – Medical Questionnaire file. This label indicates if “You were ever told you had cancer or malignancy”.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mapped the classes to 0 – No Cancer (Majority) and 1 -  Cancer (Minority).&lt;/li&gt;
  &lt;li&gt;There are 33,074 records with 59 features and ~9% are in the minority class (Cancer).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Random Forest:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class_weight hyperparameter was heavily weighted towards the minority class for the regular and downsampled data {0: 0.1, 1: 0.9}.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 15, since it has more data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Cancer)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.36&lt;/li&gt;
      &lt;li&gt;Precision: 0.24&lt;/li&gt;
      &lt;li&gt;Recall: 0.66&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A cutoff of 0.55 instead of 0.50 for classification actually improved the f1-score of both classes by 0.02.&lt;/p&gt;

&lt;p&gt;The output of feature importance for Random Forest (Left) and XGBoost (Right):&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P3_CA_RFXG.png&quot; alt=&quot;P3_CA_RFXG&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P3_CA_RFXG.png&quot;&gt; &lt;img src=&quot;../images/practicum/P3_CA_RFXG.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;XGBoost:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scale_pos_weight hyperparameter was weighted towards the minority class with a value of 8 for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 6 compared to 4 since it has more data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Cancer)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.36&lt;/li&gt;
      &lt;li&gt;Precision: 0.25&lt;/li&gt;
      &lt;li&gt;Recall: 0.66&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;XGBoost versus Random Forest:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The max depth of the trees is much less than random forest at 6 vs 15 for the upsample.&lt;/li&gt;
  &lt;li&gt;XGBoost had the same f1-score for the minority class and only improved precision of the minority class slightly by 0.01.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The following combines the weighted feature importance of the algorithms to identify the risk factors of cancer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P3_CA_RF.png&quot; alt=&quot;P3_CA_RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This shows that some of the important risk factors for cancer are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;RIDAGEYR – Age at Screening&lt;/li&gt;
  &lt;li&gt;RIDRETH1_3.0 – Non-Hispanic White (0 – Yes, 1 – No)&lt;/li&gt;
  &lt;li&gt;BPQ020_2.0 – Ever told you had high blood pressure (0 – Yes, 1 – No)&lt;/li&gt;
  &lt;li&gt;HID010_2.0 – Covered by health insurance (0 – Yes, 1 - No)&lt;/li&gt;
  &lt;li&gt;DMDHHSIZ – Number of people in the household (1 to 7 or more)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some of the features like HUQ050 (# Time Received Healthcare) is a result of the disease, so it’s not considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevalence of Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The prevalence of Cancer and its risk factors are depicted visually using ggplot2 in R.&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P3_CA_V1.png&quot; alt=&quot;P3_CA_V1&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P3_CA_V1.png&quot;&gt; &lt;img src=&quot;../images/practicum/P3_CA_V1.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is higher prevalence of cancer for non-Hispanic whites&lt;/li&gt;
  &lt;li&gt;There is higher prevalence of cancer in households with two members. The higher prevalence of cancer for two household members may be due to the fact that average household size is about 2.6 people.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P3_CA_V2.png&quot; alt=&quot;P3_CA_V2&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P3_CA_V2.png&quot;&gt; &lt;img src=&quot;../images/practicum/P3_CA_V2.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is higher prevalence of cancer in women than in men.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P3_CA_V3.png&quot; alt=&quot;P3_CA_V3&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P3_CA_V3.png&quot;&gt; &lt;img src=&quot;../images/practicum/P3_CA_V3.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cancer affects mostly older individuals over the age of 64, more specifically, 66 to 75 years of age.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cancer, Hospital Utilization &amp;amp; Proportion of No Healthcare Access over Time:&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P3_CA_Time.png&quot; alt=&quot;P3_CA_Time&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P3_CA_Time.png&quot;&gt; &lt;img src=&quot;../images/practicum/P3_CA_Time.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The plot of cancer over time shows that the prevalence is increasing.&lt;/li&gt;
  &lt;li&gt;Among those who responded to the cancer questionnaire, there is an increase in hospital utilization, especially from 2009 to 2016, with the highest in 2016.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P3_CA_Time2.png&quot; alt=&quot;P3_CA_Time2&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Socio-economic factors such as proportion of individuals with no healthcare access who have cancer has increased over the time period from 2011 to 2016 as well.&lt;/li&gt;
  &lt;li&gt;Also, not shown here, but prevalence of low household income levels and low education levels have decreased over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;part-4-chronic-lung-disease&quot;&gt;Part 4: Chronic Lung Disease&lt;/h3&gt;
&lt;p&gt;To see different risk factors associated with Chronic Lung Disease, the label ‘MCQ160K’ is used in the MCQ – Medical Questionnaire file. This label indicates if “You were ever told you had bronchitis”.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mapped the classes to 0 – No Chronic Lung Disease (Majority) and 1 -  Chronic Lung Disease (Minority).&lt;/li&gt;
  &lt;li&gt;There are 33,009 records with 60 features and ~6% are in the minority class (Chronic Lung Disease).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Random Forest:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class_weight hyperparameter was heavily weighted towards the minority class for the regular and downsampled data {0: 0.1, 1: 0.9}.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 15, since it has more balanced data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Chronic Lung Disease)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.31&lt;/li&gt;
      &lt;li&gt;Precision: 0.28&lt;/li&gt;
      &lt;li&gt;Recall: 0.36&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Changing cutoffs for probabilities did not improve the f1-score.&lt;/p&gt;

&lt;p&gt;The output of feature importance for Random Forest (Left) and XGBoost (Right):&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P4_CL_RFXG.png&quot; alt=&quot;P4_CL_RFXG&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P4_CL_RFXG.png&quot;&gt; &lt;img src=&quot;../images/practicum/P4_CL_RFXG.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;XGBoost:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scale_pos_weight hyperparameter was weighted towards the minority class with a value of 8 for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 6 compared to 3 since it has more balanced data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Chronic Lung Disease)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.31&lt;/li&gt;
      &lt;li&gt;Precision: 0.23&lt;/li&gt;
      &lt;li&gt;Recall: 0.45&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;XGBoost versus Random Forest:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The max depth of the trees is much less than random forest at 6 vs 15.&lt;/li&gt;
  &lt;li&gt;XGBoost had the same f1-score for the minority class and improved recall by 0.09, but did worse in precision by 0.05.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The following combines the weighted feature importance of the algorithms to identify the risk factors of Chronic Lung Disease.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P4_CL_RF.png&quot; alt=&quot;P4_CL_RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This shows that some of the important risk factors for Chronic Lung Disease are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;MCQ010_2.0 – Ever been told you have asthma&lt;/li&gt;
  &lt;li&gt;RIDAGEYR – Age at Screening&lt;/li&gt;
  &lt;li&gt;RIAGENDR_2.0 – Male or Female (1 – Female)&lt;/li&gt;
  &lt;li&gt;DMDBORN4_2.0 – Country of birth (1 – Other, 0 – U.S.)&lt;/li&gt;
  &lt;li&gt;SMD410_2.0 – Does anyone smoke in the home? (0-Yes, 1-No)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some of the features like HUQ010 (General health Condition) and HUQ050 (# Time Received Healthcare) are a result of the disease, so it’s not considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevalence of Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The prevalence of Chronic Lung Disease and its risk factors are depicted visually using ggplot2 in R.&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P4_CL_V1.png&quot; alt=&quot;P4_CL_V1&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P4_CL_V1.png&quot;&gt; &lt;img src=&quot;../images/practicum/P4_CL_V1.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The prevalence of chronic lung disease for those with asthma is higher compared to those without. That could be a reason why it’s number one in feature importance.&lt;/li&gt;
  &lt;li&gt;Chronic lung disease primarily affects individuals between the ages of 49 to 63.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P4_CL_V2.png&quot; alt=&quot;P4_CL_V2&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P4_CL_V2.png&quot;&gt; &lt;img src=&quot;../images/practicum/P4_CL_V2.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Females are more likely to be affected by chronic lung disease than males.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P4_CL_V3.png&quot; alt=&quot;P4_CL_V3&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The prevalence of chronic lung disease for those with smoking in the household is very high compared to those without.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Chronic Lung Disease Over Time:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P4_CL_Time.png&quot; alt=&quot;P4_CL_Time&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There doesn’t seem to be any upward or downward trend in chronic lung disease over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P3_CA_Time2.png&quot; alt=&quot;P3_CA_Time2&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Socio-economic factors such as proportion of individuals with no healthcare access who have chronic lung disease has increased over the time period from 2011 to 2016 as well.&lt;/li&gt;
  &lt;li&gt;Also, not shown here, but prevalence of low household income levels and low education levels have decreased over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;part-5-diabetes&quot;&gt;Part 5: Diabetes&lt;/h3&gt;
&lt;p&gt;To see different risk factors associated with Diabetes, the label ‘DIQ010’ is used in the DIQ – Diabetes Questionnaire file. This label indicates if “Doctor told you have diabetes”.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mapped the classes to 0 – No Diabetes (Majority) and 1 -  Diabetes (Minority).&lt;/li&gt;
  &lt;li&gt;There are 15,105 records with 61 features and ~12% are in the minority class (Diabetes).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Random Forest:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class_weight hyperparameter was heavily weighted towards the minority class for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 15, since it has more balanced data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Diabetes)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.47&lt;/li&gt;
      &lt;li&gt;Precision: 0.36&lt;/li&gt;
      &lt;li&gt;Recall: 0.67&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Changing cutoffs for probabilities did not improve the f1-score.&lt;/p&gt;

&lt;p&gt;The output of feature importance for Random Forest (Left) and XGBoost (Right):&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P5_DI_RFXG.png&quot; alt=&quot;P5_DI_RFXG&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P5_DI_RFXG.png&quot;&gt; &lt;img src=&quot;../images/practicum/P5_DI_RFXG.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;XGBoost:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scale_pos_weight hyperparameter was weighted towards the minority class with a value of 8 for the regular and downsampled data.&lt;/li&gt;
  &lt;li&gt;The upsampled data required a larger max_depth at 6 compared to 4 since it  more balanced data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;F1 Score:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regular data produced the best F1 Score on minority class (Diabetes)
    &lt;ul&gt;
      &lt;li&gt;F1 Score: 0.49&lt;/li&gt;
      &lt;li&gt;Precision: 0.38&lt;/li&gt;
      &lt;li&gt;Recall: 0.71&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;XGBoost versus Random Forest:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The max depth of the trees is much less than random forest at 6 vs 15 and 4 vs 8 for regular and downsampled.&lt;/li&gt;
  &lt;li&gt;XGBoost improved f1-score of the minority class by 0.02 by increasing precision by 0.02 and recall by 0.04.&lt;/li&gt;
  &lt;li&gt;XGBoost performs better!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The following combines the weighted feature importance of the algorithms to identify the risk factors of Diabetes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P5_DI_RF.png&quot; alt=&quot;P5_DI_RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This shows that some of the important risk factors for Diabetes are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;RIDAGEYR – Age at Screening&lt;/li&gt;
  &lt;li&gt;BPQ020_2.0 – Ever told you had high blood pressure (0 – Yes, 1 – No)&lt;/li&gt;
  &lt;li&gt;BMXBMI – Body Mass Index&lt;/li&gt;
  &lt;li&gt;LBXTC – Total cholesterol (mg/dL)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some of the features like HUQ010 (General health Condition) and HUQ050 (# Time Received Healthcare) are a result of the disease, so it’s not considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevalence of Risk Factors:&lt;/strong&gt;&lt;br /&gt;
The prevalence of Diabetes and its risk factors are depicted visually using ggplot2 in R.&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P5_DI_V1.png&quot; alt=&quot;P5_DI_V1&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P5_DI_V1.png&quot;&gt; &lt;img src=&quot;../images/practicum/P5_DI_V1.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Those who have high blood pressure also have a high prevalence of diabetes.&lt;/li&gt;
  &lt;li&gt;As Body Mass Index increases, the prevalence of Diabetes increases substantially. BMI is highly correlated to diabetes.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P5_DI_V2.png&quot; alt=&quot;P5_DI_V2&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P5_DI_V2.png&quot;&gt; &lt;img src=&quot;../images/practicum/P5_DI_V2.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Diabetes is prevalent among individuals ages 49 and over.&lt;/li&gt;
  &lt;li&gt;Most individuals who have diabetes have desirable total cholesterol levels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Diabetes, Hospital Utilization and Proportion of No Healthcare Access Over Time:&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P5_DI_Time.png&quot; alt=&quot;P5_DI_Time&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P5_DI_Time.png&quot;&gt; &lt;img src=&quot;../images/practicum/P5_DI_Time.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Diabetes has an increasing trend every single year from 2003 and onwards.&lt;/li&gt;
  &lt;li&gt;Hospital utilization has increased for those who answered the diabetes questionnaire.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/P5_DI_Time2.png&quot; alt=&quot;P5_DI_Time2&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Socio-economic factors such as proportion of individuals with no healthcare access who have diabetes has increased over the time period from 2011 to 2016 as well.&lt;/li&gt;
  &lt;li&gt;Also, not shown here, but prevalence of low household income levels and low education levels have decreased over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;undiagnosed-diseases&quot;&gt;Undiagnosed Diseases&lt;/h2&gt;
&lt;h3 id=&quot;part-6-undiagnosed-diabetes-bmi--30&quot;&gt;Part 6: Undiagnosed Diabetes (BMI &amp;gt; 30)&lt;/h3&gt;
&lt;p&gt;Question 5: Has the prevalence of undiagnosed conditions increased in rural populations over time, compared to non-rural populations?&lt;/p&gt;

&lt;p&gt;From Part 5, since BMI levels are highly correlated to risk of diabetes, I will use BMI levels to visualize and see whether the prevalence of undiagnosed diabetes has increased in rural populations over time compared to non-rural populations.&lt;br /&gt;
Individuals who have BMI levels greater than 30 are considered to have diabetes. I created a new column in my data with a value of “Undiagnosed Diabetes” or “Not Undiagnosed”. An individual who is coded “Undiagnosed Diabetes” has a value of 0 for DIQ010, indicating no doctor told them they have diabetes, and they also have a value &amp;gt;30 for BMXBMI. The graph below shows the error box plots of the interaction and prevalence between “Undiagnosed Diabetes” and “Year” over time from 1999 to 2016.&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/practicum/P6_UNDIAG.png&quot; alt=&quot;P6_UNDIAG&quot; width=&quot;450px&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/practicum/P6_UNDIAG.png&quot;&gt; &lt;img src=&quot;../images/practicum/P6_UNDIAG.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The plot shows that the prevalence of undiagnosed diabetes has indeed increased over time.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Hospital Utilization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hospital utilization overall hasn’t increased much over time, but for individuals who responded to the cancer and diabetes questionnaire, hospital utilization has increased. Hospital utilization for those who have heart disease seems to have increased slightly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Age is a huge factor in determining hospital utilization and risk of major diseases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Major Diseases:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/practicum/Conclusion.png&quot; alt=&quot;Conclusion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Heart Disease&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Risk factors include age, blood pressure, gender, and cholesterol.&lt;/li&gt;
  &lt;li&gt;Prevalence of heart disease is high in individuals with desirable levels of cholesterol, so there may be some undiagnosed individuals.&lt;/li&gt;
  &lt;li&gt;Heart disease is more prevalent in males and those with high blood pressure.&lt;/li&gt;
  &lt;li&gt;Heart disease prevalence isn’t increasing over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cancer&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Risk factors include gender, being a non-Hispanic white, household size, and age.&lt;/li&gt;
  &lt;li&gt;Being female, having a household size of two, and non-Hispanic whites are indicative of a high prevalence of cancer.&lt;/li&gt;
  &lt;li&gt;The prevalence of cancer is increasing a lot over time.
    &lt;ul&gt;
      &lt;li&gt;For individuals who repsonded to the cancer questionnaire, hospital utilization has also increased.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Chronic Lung Disease&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Risk factors include asthma, age, gender, and smoking.&lt;/li&gt;
  &lt;li&gt;Prevalence is high for people with asthma and it’s almost double for females compared to males.&lt;/li&gt;
  &lt;li&gt;The prevalence of chronic lung disease isn’t increasing over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Diabetes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Risk factors for diabetes include body mass index, age, blood pressure, and cholesterol.&lt;/li&gt;
  &lt;li&gt;High body mass index is highly correlated to diabetes.&lt;/li&gt;
  &lt;li&gt;High blood pressure is also a good indicator of diabetes.&lt;/li&gt;
  &lt;li&gt;The prevalence of diabetes is increasing a lot over time.
    &lt;ul&gt;
      &lt;li&gt;For individuals who repsonded to the diabetes questionnaire, hospital utilization has also increased over time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Socio-economic Factors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Socio-economic factors such as proportion of individuals with no healthcare access who have diabetes and chronic lung disease have increased over the time period from 2011 to 2016 as well.&lt;/li&gt;
  &lt;li&gt;Also, not shown here, but prevalence of low household income levels and low education levels have actually decreased over time.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justin Gong</name></author><category term="practicum" /><category term="python" /><category term="R" /><category term="supervised learning" /><summary type="html">Supervised Learning, R (ggplot2, survey), Python, XGBoost, Random Forest, Sci-kit Learn, Pandas</summary></entry><entry><title type="html">Supervised Learning</title><link href="http://localhost:4000/sl/" rel="alternate" type="text/html" title="Supervised Learning" /><published>2018-06-03T00:00:00-07:00</published><updated>2018-06-03T00:00:00-07:00</updated><id>http://localhost:4000/sl</id><content type="html" xml:base="http://localhost:4000/sl/">&lt;p&gt;Details for this project are available on &lt;a href=&quot;https://github.com/jjgong7/Machine-Learning/tree/master/1%20-%20Supervised%20Learning&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective:&lt;/h2&gt;

&lt;p&gt;Analyze two datasets using five different supervised learning algorithms. Weka, a suite of machine learning software written in Java, is used for analysis.&lt;/p&gt;

&lt;h2 id=&quot;datasets-used&quot;&gt;Datasets used:&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Faulty Steel Plates&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1,941 instances; 27 attributes; 7 labels&lt;/li&gt;
  &lt;li&gt;Steel Plates are classified into 7 different faulty categories - Pastry, Z_Scratch, K_Scratch, Stains, Dirtiness, Bumps, and Other_Faults&lt;/li&gt;
  &lt;li&gt;Other_Faults ~ 35%, Bumps ~ 20%, K_Scratch ~ 20%&lt;/li&gt;
  &lt;li&gt;All features are numerical&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;11,055 instances, 30 attributes, binary label&lt;/li&gt;
  &lt;li&gt;Websites are classified as a phishing website (1) or not a phishing website (-1)&lt;/li&gt;
  &lt;li&gt;Not Phishing ~ 44%, Phishing ~ 56%&lt;/li&gt;
  &lt;li&gt;All data is nominal (-1, 0, or 1)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-are-the-datasets-interesting&quot;&gt;Why are the datasets interesting?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Faulty Steel Plates&lt;/strong&gt;&lt;br /&gt;
Identifying faulty steel plates can:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Improve safety and reduce costs (return fees)&lt;/li&gt;
  &lt;li&gt;Reduce amount of defective plates used and in circulation&lt;/li&gt;
  &lt;li&gt;Applicable to evaluating other types of defective metals&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites&lt;/strong&gt;&lt;br /&gt;
Identifying phishing websites can:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Improve online security&lt;/li&gt;
  &lt;li&gt;Prevents identity theft, credit card theft, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;analysis-steps&quot;&gt;Analysis Steps:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;80% training data, 20% test data&lt;/li&gt;
  &lt;li&gt;10-fold cross validation used on training set to help generalize and avoid overfitting&lt;/li&gt;
  &lt;li&gt;Accuracy rate for training and cross-validation is averaged accross folds&lt;/li&gt;
  &lt;li&gt;Training set (build the model), validation set (tune the hyperparameters and pick a model), and held-out test set (final performance of the chosen model)&lt;/li&gt;
  &lt;li&gt;Two learning curves graphed
    &lt;ul&gt;
      &lt;li&gt;Training and Validation Accuracy vs. Training Size (Learning Curve)
        &lt;ul&gt;
          &lt;li&gt;Used to evaluate bias and variance. Training size in increments of 10%&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Training and Validation Accuracy vs. Range of Single Hyperparameter Value (Model Evaluation)
        &lt;ul&gt;
          &lt;li&gt;Performance of training and cv accuracy vary for different inputs of a single hyperparameter&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Supervised Learning Algorithms:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Decision Tree&lt;/li&gt;
  &lt;li&gt;Boosting&lt;/li&gt;
  &lt;li&gt;Neural Network&lt;/li&gt;
  &lt;li&gt;Support Vector Machines&lt;/li&gt;
  &lt;li&gt;k-Nearest Neighbors (kNN)&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;1-decision-tree&quot;&gt;1. Decision Tree:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/dt.jpg&quot; alt=&quot;Decision Tree&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a top down learning algorithm that splits the best attributes (decision nodes) based on information gain. The branches are the outcomes of the binary splits (≥,≤ or T, F) and the leaf nodes represent a classifying label.&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pruning (True, False)
    &lt;ul&gt;
      &lt;li&gt;Pruning serves to simplify the tree, making it easier to understand the results and avoid risk of overfitting the training data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Confidence Factor (Default = 0.25)
    &lt;ul&gt;
      &lt;li&gt;Lower values incur more pruning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: Decision Tree&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/DT_LC_FP.png&quot; alt=&quot;DT_LC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;High Variance/Low Bias (Overfitting)&lt;/li&gt;
  &lt;li&gt;Training accuracy ~93% while validation accuracy ~70%&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/DT_MC_FP.png&quot; alt=&quot;DT_MC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Adjusted confidence factor from 0.1 to 0.5&lt;/li&gt;
  &lt;li&gt;With confidence factor = 0.2, tree size is 257 with 129 leaves, cv-accuracy is 73.32%.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites: Decision Tree&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-1&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/DT_LC_PW.png&quot; alt=&quot;DT_LC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Low Variance/Low Bias (Good Model)&lt;/li&gt;
  &lt;li&gt;Accuracy is high for both training and validation set&lt;/li&gt;
  &lt;li&gt;Classifies well even with low training data (~10%)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-1&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/DT_MC_PW.png&quot; alt=&quot;DT_MC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;With confidence factor = 0.5, tree size is 432, and has second highest cv-accuracy after the unpruned tree.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Phishing websites has a much better classification accuracy than faulty plates. This can be because faulty plates dataset has seven different class labels, while phishing websites only has a binary label.&lt;/p&gt;

&lt;h3 id=&quot;2-boosting&quot;&gt;2. Boosting:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/bs.png&quot; alt=&quot;Boosting&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Boosting is an ensemble learning method that incrementally builds the model by placing more weight on the misclassified data. It basically turns a set of weak learners, which always do better than chance, into a single strong learner. In Weka, the boosting algorithm is called AdaBoostM1.&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Defaults:
    &lt;ul&gt;
      &lt;li&gt;Base Classifier (J48 Decision Tree)&lt;/li&gt;
      &lt;li&gt;Pruned Tree&lt;/li&gt;
      &lt;li&gt;0.25 Confidence Factor&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Number of Iterations:
    &lt;ul&gt;
      &lt;li&gt;Higher iterations use more weak learners&lt;/li&gt;
      &lt;li&gt;10 to 50&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: Boosting&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-2&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/BS_LC_FP.png&quot; alt=&quot;BS_LC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;High Variance/Low Bias (Overfitting)&lt;/li&gt;
  &lt;li&gt;100% accuracy on training set while validation set only has around 71% accuracy, so it doesn’t generalize well&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-2&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/BS_MC_FP.png&quot; alt=&quot;BS_MC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Test iterations from 10 to 50 to improve weak learners&lt;/li&gt;
  &lt;li&gt;Best cv-accuracy at 79.70% when iterations = 40. Tree size is 293 with 147 leaves.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites: Boosting&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-3&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/BS_LC_PW.png&quot; alt=&quot;BS_LC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Low Variance/Low Bias (Good model)&lt;/li&gt;
  &lt;li&gt;Training and validation data are both above ~92%&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-3&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/BS_MC_PW.png&quot; alt=&quot;BS_MC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Test iterations from 10 to 50 to improve weak learners&lt;/li&gt;
  &lt;li&gt;Best cv-accuracy at 96.96% when iterations = 30. Tree size is lowest at 19.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the learning curves, both datasets improve a lot with more training data:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Faulty Plates from 64% to 79%&lt;/li&gt;
  &lt;li&gt;Phishing Websites from 92.5% to 97.04%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This shows with more training data, the model is able to generalize better and achieve a lower variance/bias. Boosting does better than the original decision tree because it improves upon our decision tree base classifier by turning weak learners into a set of strong learners.&lt;/p&gt;

&lt;h3 id=&quot;3-neural-network&quot;&gt;3. Neural Network:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/nn.jpeg&quot; alt=&quot;Neural Network&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A neural network tries to mimic the brain in mapping inputs to outputs. Weka uses MultiLayerPerceptron - a network of perceptions.&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hidden Layers
    &lt;ul&gt;
      &lt;li&gt;Default of one hidden layer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Number of Neurons
    &lt;ul&gt;
      &lt;li&gt;Default (attributes + classes)/2 neurons in each layer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Weights (Learning Rate, Momentum)
    &lt;ul&gt;
      &lt;li&gt;Weights for the perceptrons are learned from the training set and updated via “backpropagation”.&lt;/li&gt;
      &lt;li&gt;The change in weight is the learning rate times the gradient plus the previous change in weight times the momentum.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: Neural Network&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-4&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/NN_LC_FP.png&quot; alt=&quot;NN_LC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;High Variance/Low Bias (Overfitting)&lt;/li&gt;
  &lt;li&gt;Variance decreases as more of the data is used for training since training and cv-accuracy slightly converge.&lt;/li&gt;
  &lt;li&gt;Bias increases slightly as the model performs worse on training set.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-4&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/NN_MC_FP.png&quot; alt=&quot;NN_MC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Altered amount of neurons for hidden layer, optimal was 16 neurons = [(attributes+classes)/2] neurons&lt;/li&gt;
  &lt;li&gt;As number of hidden layers increased, the cv-accuracy decreased&lt;/li&gt;
  &lt;li&gt;Altered learning rate (default=0.2) and momentum (default=0.3)
    &lt;ul&gt;
      &lt;li&gt;Lower rates produced better results (smaller steps in calculating the weight change), but also longer training time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimal: 1 hidden layer, 16 neurons, and learning rate and momentum of 0.1
    &lt;ul&gt;
      &lt;li&gt;cv-accuracy of 72.87%&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites: Neural Network&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-5&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/NN_LC_PW.png&quot; alt=&quot;NN_LC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Low Variance/Low Bias (Good model)&lt;/li&gt;
  &lt;li&gt;Variance decreases with more training data&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-5&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/NN_MC_PW.png&quot; alt=&quot;NN_MC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Altered amount of neurons for hidden layer, optimal was 16 neurons = [(attributes+classes)/2] neurons&lt;/li&gt;
  &lt;li&gt;As number of hidden layers increased, the cv-accuracy decreased&lt;/li&gt;
  &lt;li&gt;Altered learning rate (default=0.2) and momentum (default=0.3)
    &lt;ul&gt;
      &lt;li&gt;Lower rates produced better results (smaller steps in calculating the weight change), but also longer training time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimal: 1 hidden layer, 16 neurons, and learning rate and momentum of 0.1
    &lt;ul&gt;
      &lt;li&gt;cv-accuracy of 96.69%&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4-support-vector-machines-svm&quot;&gt;4. Support Vector Machines (SVM):&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/svm.png&quot; alt=&quot;SVM&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Support Vector Machines uses hyperplanes to maximize the margins between the different classes. The larger the distance between the hyperplanes, the better the generalization and separation of classes. In Weka, LibSVM is the most popular and robust SVM algorithm. LibSVM uses one vs one classification, where each different pair of labels has a separately trained classifier.&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Kernels
    &lt;ul&gt;
      &lt;li&gt;Linear, Polynomial, Radial Basis Function, Sigmoid&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: SVM&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-6&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/SVM_LC_FP.png&quot; alt=&quot;SVM_LC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;High Variance/Low Bias (Overfitting)&lt;/li&gt;
  &lt;li&gt;Training set has 100% accuracy, while the validation set only performs around 62% - the worst performing algorithm yet!&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-6&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/SVM_MC_FP.png&quot; alt=&quot;SVM_MC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Linear kernel performs best with a cv-accuracy of 69.91%&lt;/li&gt;
  &lt;li&gt;Poorest performing algorithm for faulty plates, most likely because the hyperplanes aren’t able to separate the seven classes well. It performs better on binary classification.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites: SVM&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-7&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/SVM_LC_PW.png&quot; alt=&quot;SVM_LC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Low Variance/Low Bias&lt;/li&gt;
  &lt;li&gt;Training accuracy and cv-accuracy are almost identical&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-7&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/SVM_MC_PW.png&quot; alt=&quot;SVM_MC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Radial Basis Function kernel with default hyperparameters performs best with ~94% accuracy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the learning curves, both models generalize better as the training set increases.&lt;/p&gt;

&lt;h3 id=&quot;5-k-nearest-neighbors-knn&quot;&gt;5. k-Nearest Neighbors (kNN):&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/knn.png&quot; alt=&quot;KNN&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The k-Nearest Neighbor algorithm outputs a classification based on the majority of the number of neighbors. This is an instance based, lazy learner, which means it does nothing until you have to make a prediction. In Weka, IBK is used.&lt;/p&gt;

&lt;p&gt;Hyperparameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of Nearest Neighbors
    &lt;ul&gt;
      &lt;li&gt;Default: 1 Nearest Neighbor&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Distance Function
    &lt;ul&gt;
      &lt;li&gt;Default: Euclidean Distance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: kNN&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-8&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/KNN_LC_FP.png&quot; alt=&quot;KNN_LC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;High Variance/Low Bias (Overfitting)&lt;/li&gt;
  &lt;li&gt;Training set shows 100% accuracy, while cv-accuracy is 71%&lt;/li&gt;
  &lt;li&gt;Model doesn’t improve much with more training data&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-8&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/KNN_MC_FP.png&quot; alt=&quot;KNN_MC_FP&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of nearest neighbors adjusted from 1 to 20
    &lt;ul&gt;
      &lt;li&gt;As number of neighbors increases, more bias is introduced because the cv-accuracy and training accuracy get worse.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimal number of neighbors is k=5, with 71.59% cv-accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Phishing Websites: kNN&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;learning-curve-9&quot;&gt;Learning Curve&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/KNN_LC_PW.png&quot; alt=&quot;KNN_LC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Low Variance/Low Bias&lt;/li&gt;
  &lt;li&gt;Model improves significantly with more training data, cv-accuracy improves by over 6% from 10% training data to 100%. Variance decreases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-complexity-9&quot;&gt;Model Complexity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/KNN_MC_PW.png&quot; alt=&quot;KNN_MC_PW&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of nearest neighbors adjusted from 1 to 20
    &lt;ul&gt;
      &lt;li&gt;As number of neighbors increases, more bias is introduced because the cv-accuracy and training accuracy get worse.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimal number of neighbors is k=1, with 97.04% cv-accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/ml-sup/Conclusion.png&quot; alt=&quot;Summary&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;After hyperparameter tuning and selecting the best hyperparameters, test set is used to determine the final performance and quality of the models.&lt;/li&gt;
  &lt;li&gt;Accuracy and confusion matrix were used to determine the best models&lt;/li&gt;
  &lt;li&gt;Faulty Steel Plates Dataset:
    &lt;ul&gt;
      &lt;li&gt;Boosting Algorithm with Decision Tree Base Classifier produces the best test accuracy at 77.12%. Decision Tree was second.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Phishing Websites Dataset:
    &lt;ul&gt;
      &lt;li&gt;Decision Tree, Boosting, and Multilayer Perceptron all do well with an accuracy of ~96%&lt;/li&gt;
      &lt;li&gt;KNN performs best on the test set with 97.06% accuracy&lt;/li&gt;
      &lt;li&gt;KNN also a fast algorithm since it’s a lazy learner that only tests for 5 nearest neighbors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justin Gong</name></author><category term="supervised learning" /><category term="python" /><summary type="html">Supervised Learning, Decision Trees, Boosting, Neural Networks, Support Vector Machines, k-Nearest Neighbors</summary></entry><entry><title type="html">Unsupervised Learning</title><link href="http://localhost:4000/ul/" rel="alternate" type="text/html" title="Unsupervised Learning" /><published>2018-06-02T00:00:00-07:00</published><updated>2018-06-02T00:00:00-07:00</updated><id>http://localhost:4000/ul</id><content type="html" xml:base="http://localhost:4000/ul/">&lt;p&gt;Details for this project are available on &lt;a href=&quot;https://github.com/jjgong7/Machine-Learning/tree/master/3%20-%20Unsupervised%20Learning%20and%20Dimensionality%20Reduction&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective:&lt;/h2&gt;

&lt;p&gt;Six different algorithms are implemented; the first two are clustering – k-means clustering and Expectation Maximization and the last four are dimensionality reduction algorithms – PCA, ICA, Randomized Projections, and Random Forest. The experiments are split into four main parts. All algorithms are evaluated using Scikit-learn in Python.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Part 1:
    &lt;ul&gt;
      &lt;li&gt;Applies clustering on both datasets&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Part 2:
    &lt;ul&gt;
      &lt;li&gt;Applies dimensionality reduction algorithms on both datasets&lt;/li&gt;
      &lt;li&gt;Reproduces clustering experiments with dimensionality reduction on both datasets&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Part 3:
    &lt;ul&gt;
      &lt;li&gt;Applies dimensionality reduction algorithms to the Faulty Plates datasets&lt;/li&gt;
      &lt;li&gt;Run Neural Network on the data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Part 4:
    &lt;ul&gt;
      &lt;li&gt;Applies clustering algorithms and uses the clusters as new features for the Neural Network on the Faulty Plates dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets-used&quot;&gt;Datasets used:&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Faulty Steel Plates&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1,941 instances; 27 attributes; 7 labels&lt;/li&gt;
  &lt;li&gt;Steel Plates are classified into 7 different faulty categories - Pastry, Z_Scratch, K_Scratch, Stains, Dirtiness, Bumps, and Other_Faults&lt;/li&gt;
  &lt;li&gt;Other_Faults ~ 35%, Bumps ~ 20%, K_Scratch ~ 20%&lt;/li&gt;
  &lt;li&gt;All features are numerical&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;569 instances, 31 attributes, binary label&lt;/li&gt;
  &lt;li&gt;Features are measurements calculated from a digitized image of a breast mass&lt;/li&gt;
  &lt;li&gt;Classifies whether a breast mass is malignant or benign&lt;/li&gt;
  &lt;li&gt;357 Benign, 212 Malignant&lt;/li&gt;
  &lt;li&gt;All features are real valued&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-are-the-datasets-interesting&quot;&gt;Why are the datasets interesting?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Faulty Steel Plates&lt;/strong&gt;&lt;br /&gt;
Identifying faulty steel plates:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Improve safety and reduce costs (return fees)&lt;/li&gt;
  &lt;li&gt;Reduce amount of defective plates used and in circulation&lt;/li&gt;
  &lt;li&gt;Applicable to evaluating other types of defective metals&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer&lt;/strong&gt;&lt;br /&gt;
Identifying breast cancer:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Breast Cancer affects about 200,000 women a year in the U.S.&lt;/li&gt;
  &lt;li&gt;About 12% of U.S. women develop breast cancer in their lifteime&lt;/li&gt;
  &lt;li&gt;Early identification of malignant breast mass cells using machine learning is very beneficial for prevention, treatment, and potentially saves lives.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Algorithms Used:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Clustering (k-means and EM)&lt;/li&gt;
  &lt;li&gt;Neural Network&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Dimensionality Reduction&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Principal Component Analysis (PCA)&lt;/li&gt;
  &lt;li&gt;Independent Component Analysis (ICA)&lt;/li&gt;
  &lt;li&gt;Random Projections (RP)&lt;/li&gt;
  &lt;li&gt;Random Forest (RF)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;part-1-clustering&quot;&gt;Part 1: Clustering&lt;/h2&gt;
&lt;p&gt;Clustering is an unsupervised learning algorithm that groups a set of observations together that are similar to each other compared to those in other groups.&lt;/p&gt;

&lt;p&gt;K-Means:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Takes in a parameter k, for the amount of clusters, and randomly generates k means&lt;/li&gt;
  &lt;li&gt;K-clusters are formed based on associating each observation to the closest mean&lt;/li&gt;
  &lt;li&gt;The least squared Euclidean distance is used for measurement&lt;/li&gt;
  &lt;li&gt;The center of each of the clusters becomes the new mean&lt;/li&gt;
  &lt;li&gt;These steps are iterated until convergence is reached&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Expectation-Maximization:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Iterative method using maximum likelihood to find the clusters means&lt;/li&gt;
  &lt;li&gt;Alternates between a soft clustering (Expectation) and computing the means of a soft cluster (Maximization)&lt;/li&gt;
  &lt;li&gt;Expectation calculates the likelihood that the observation is in a certain cluster based on the mean&lt;/li&gt;
  &lt;li&gt;Maximization computes the means from likelihoods, using weighted averages of the data points&lt;/li&gt;
  &lt;li&gt;Each step maximizes the likelihood of the distribution until convergence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;To find a good k:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means:
    &lt;ul&gt;
      &lt;li&gt;Sum of Squared Distances within clusters vs # of clusters
        &lt;ul&gt;
          &lt;li&gt;Elbow point where SSE decreases sharply, can be used to determine k&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Expectation-Maximization:
    &lt;ul&gt;
      &lt;li&gt;Log-likelihood
        &lt;ul&gt;
          &lt;li&gt;Calculates the likelihood that the data is to be generated by the parameters estimated&lt;/li&gt;
          &lt;li&gt;Higher likelihood means that the data is more likely to be generated by the estimated parameters (tradeoff with overfitting)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Both:
    &lt;ul&gt;
      &lt;li&gt;Silhouette Score:
        &lt;ul&gt;
          &lt;li&gt;Takes into account both intra and inter cluster distances
            &lt;ul&gt;
              &lt;li&gt;Explains how similar an observation is to its own cluster compared to other clusters.&lt;/li&gt;
              &lt;li&gt;Range is from -1 to 1
                &lt;ul&gt;
                  &lt;li&gt;1 best value meaning it matches well with its own cluster and far from other clusters&lt;/li&gt;
                  &lt;li&gt;0 indicates overlapping clusters&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Evaluation of clusters:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Accuracy
    &lt;ul&gt;
      &lt;li&gt;Measures the percentage of the predicted label matching with the true label&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adjusted Mutual Information Score
    &lt;ul&gt;
      &lt;li&gt;Measures the similarity between two labels of the same data between two clusters&lt;/li&gt;
      &lt;li&gt;Takes into account chance&lt;/li&gt;
      &lt;li&gt;Range is from 0 to 1
        &lt;ul&gt;
          &lt;li&gt;1 means that the labels agree&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: Clustering&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;evaluation&quot;&gt;Evaluation:&lt;/h4&gt;
&lt;!-- &lt;img src=&quot;../images/ml-unsup/FP_Clustering.png&quot; alt=&quot;FP Clustering&quot;&gt;   --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/FP_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/FP_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow at k=9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Spike at k=7 and k=9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Spike at k=9 for k-means, but dip for EM&lt;/li&gt;
      &lt;li&gt;k=6 experiences a spike for both clustering methods&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Peaks at k=9 for both&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;k=7 makes sense since there are seven different labels for the faulty steel plates datasets. k=9 also makes sense because there is an “other” category that could contain two distinct faulty plates labeled as “other”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: Clustering&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;evaluation-1&quot;&gt;Evaluation:&lt;/h4&gt;
&lt;!-- &lt;img src=&quot;../images/ml-unsup/BC_Clustering.png&quot; alt=&quot;BC Clustering&quot;&gt;   --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/BC_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/BC_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Spike at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Spike at k=7 for both&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Accuracy highest at k=7 and AdjMI leveled out somewhat at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;k=7 can make sense because there could possibly be seven different types of breast mass cells that could be identified as either malignant or benign.&lt;/p&gt;

&lt;h2 id=&quot;part-2-dimensionality-reduction-and-clustering&quot;&gt;Part 2. Dimensionality Reduction and Clustering:&lt;/h2&gt;
&lt;h3 id=&quot;principal-component-analysis-pca&quot;&gt;Principal Component Analysis (PCA):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Principal component analysis uses orthogonal transformation and linear combination to identify important components that maximizes variance&lt;/li&gt;
  &lt;li&gt;PCA is used to reduce a large set of features into a subset that still contains most of the information&lt;/li&gt;
  &lt;li&gt;To determine dimensional reduction for PCA, examine:
    &lt;ul&gt;
      &lt;li&gt;The variance explained by the components&lt;/li&gt;
      &lt;li&gt;Distribution of eigenvalues&lt;/li&gt;
      &lt;li&gt;Elbow method can be used to evaluate the number of principal components to choose&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: PCA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/FP_PCA.png&quot; alt=&quot;FP PCA&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Most of the variance can be explained by the first 12 principal components&lt;/li&gt;
  &lt;li&gt;Original dataset has 27 features, now reduced to 12 principal components&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: PCA Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/FP_PCA_Clustering.png&quot; alt=&quot;FP PCA Clustering&quot;&gt;  
 --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/FP_PCA_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/FP_PCA_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 12 principal components for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Slight elbow at k=7&lt;/li&gt;
      &lt;li&gt;Reduces SSE from 24,000 to 14,000, does well reducing inter cluster error&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Slight spike at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Low silhouette score for EM shows that it doesn’t do as well as k-means in separating the clusters. Possible overlapping clusters.&lt;/li&gt;
      &lt;li&gt;Slight spike for k=7 for both&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Shows that EM has relatively good prediction accuracy compared to k-means&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good cluster at k=7&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: PCA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/BC_PCA.png&quot; alt=&quot;FP PCA&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Most of the variance is explained by the first 7 principal components&lt;/li&gt;
  &lt;li&gt;Original dataset has 30 features, now reduced to 7 principal components&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: PCA Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/BC_PCA_Clustering.png&quot; alt=&quot;FP PCA Clustering&quot;&gt;   --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/BC_PCA_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/BC_PCA_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 7 principal components for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow method doesn’t show a clear dip&lt;/li&gt;
      &lt;li&gt;SSE shows a reduction of 1/3 from original clustering error, which means PCA reduces within cluster error, but does worse intra-cluster&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Slight spike at k=6&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Low silhouette score for EM shows that it doesn’t do as well as k-means in separating the clusters&lt;/li&gt;
      &lt;li&gt;Slight spike for k=6 for k-means&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;PCA clustering doesn’t do as well accuracy wise compared to regular clustering&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good cluster at k=6&lt;/p&gt;

&lt;h3 id=&quot;independent-component-analysis-ica&quot;&gt;Independent Component Analysis (ICA):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Independent component analysis tries to decompose data into independent non-Gaussian components&lt;/li&gt;
  &lt;li&gt;Maximizes mutual information between the original data and the independent components&lt;/li&gt;
  &lt;li&gt;The sub-components are assumed to be non-Gaussian and independent from each other
Evaluation:&lt;/li&gt;
  &lt;li&gt;The number of independent components to choose can be evaluated by their kurtosis values since kurtosis measures gaussianity and ICA tries to maximize non-gaussianity&lt;/li&gt;
  &lt;li&gt;A kurtosis near 3 is gaussian, so it’s best to find a kurtosis that has the highest absolute value of the mean of the kurtosis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: ICA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/FP_ICA.png&quot; alt=&quot;FP ICA&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Kurtosis peaks at 4&lt;/li&gt;
  &lt;li&gt;Original dataset has 27 features and now 4 indpenedent components are selected. A huge reduction in dimensionality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: ICA Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/FP_ICA_Clustering.png&quot; alt=&quot;FP ICA Clustering&quot;&gt;  --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/FP_ICA_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/FP_ICA_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 4 independent components for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow at k=5&lt;/li&gt;
      &lt;li&gt;The within cluster SSE decreases significantly compared to the original dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Slight spike at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Low silhouette score for EM shows that it doesn’t do as well as k-means in separating the clusters. Possible overlapping clusters.&lt;/li&gt;
      &lt;li&gt;Slight spike for k=7 for both&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Shows that EM has relatively good prediction accuracy compared to k-means&lt;/li&gt;
      &lt;li&gt;Peaks at k=7 for both
Good cluster at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: ICA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/BC_ICA.png&quot; alt=&quot;FP ICA&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Kurtosis keeps increasing, meaning non-gaussianity increases as the number of independent components increases&lt;/li&gt;
  &lt;li&gt;Original dataset has 30 features, but 25 independent components is enough since it starts leveling off&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: ICA Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/BC_ICA_Clustering.png&quot; alt=&quot;FP ICA Clustering&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/BC_ICA_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/BC_ICA_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 25 independent components for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow method doesn’t show a clear dip&lt;/li&gt;
      &lt;li&gt;SSE decreases from the original clustering&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Clear peak at k=6 and 8&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Produces similar cluster distances for k-means and EM&lt;/li&gt;
      &lt;li&gt;Slight spike for k=6 for k-means&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;ICA clustering has less accuracy than PCA and the original dataset&lt;/li&gt;
      &lt;li&gt;EM has highest accuracy at k=7&lt;/li&gt;
      &lt;li&gt;K-means has peak at k=7
Good cluster at k=6 and k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;random-projection-rp&quot;&gt;Random Projection (RP):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Reduces dimensions randomly using a Gaussian distribution&lt;/li&gt;
  &lt;li&gt;Benefits of random projections that it’s computationally efficient and works well on low dimensions&lt;/li&gt;
  &lt;li&gt;May perform pretty poorly based on one random generation, so 10 iterations are run and averaged for evaluation.&lt;/li&gt;
  &lt;li&gt;Goal of random projection:
    &lt;ul&gt;
      &lt;li&gt;Average Pairwise Distance Correlation: preserve the pairwise distances between any two samples of the dataset, so we want to maximize the variance and average pairwise distance correlation&lt;/li&gt;
      &lt;li&gt;Average Reconstruction Error: minimize the reconstruction error, which is the squared distance between the original data and the estimate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: RP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/FP_RP.png&quot; alt=&quot;FP RP&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Average Pairwise Distance Correlation starts leveling off around 7 dimensions&lt;/li&gt;
  &lt;li&gt;The Average Reconstruction error keeps decreasing for higher dimensions
    &lt;ul&gt;
      &lt;li&gt;Tradeoff between overfitting and computational time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: RP Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/FP_RP_Clustering.png&quot; alt=&quot;FP RP Clustering&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/FP_RP_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/FP_RP_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 7 dimensions for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Slight elbow at k=5&lt;/li&gt;
      &lt;li&gt;The within cluster SSE decreases significantly compared to the original dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Slight spike at k=9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;Spike at k=7 for both&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Accuracy for EM shows that k=9 is a good cluster&lt;/li&gt;
      &lt;li&gt;Accuracy for k-means shows that k=7 is a good cluster&lt;/li&gt;
      &lt;li&gt;Accuracy for RP EM is the highest so far for all dim algorithms at k=9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good cluster at k=7&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: RP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/BC_RP.png&quot; alt=&quot;FP RP&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Average Pairwise Distance Correlation starts levling off around 7 dimensions&lt;/li&gt;
  &lt;li&gt;Average reconstruction error keeps decreasing for higher dimensions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: RP Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/BC_RP_Clustering.png&quot; alt=&quot;FP RP Clustering&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/BC_RP_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/BC_RP_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 7 dimensions for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow at k=5&lt;/li&gt;
      &lt;li&gt;SSE decreases from the original clustering due to dim reduction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Peak around k=8&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;On average has improved for k-means from the original clustering&lt;/li&gt;
      &lt;li&gt;Peak at k=8 for EM&lt;/li&gt;
      &lt;li&gt;Peak at k=5 for k-means&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Accuracy is much lower than the original dataset&lt;/li&gt;
      &lt;li&gt;Peaks at k=5&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good cluster at k=5&lt;/p&gt;

&lt;h3 id=&quot;random-forest-rf&quot;&gt;Random Forest (RF):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Strong learner that is an ensemble of weak learner decision trees&lt;/li&gt;
  &lt;li&gt;Feature selection for Random Forest is based on feature importance&lt;/li&gt;
  &lt;li&gt;Feature importance is measured by Gini importance, which is the total decrease in node impurity reaching that node averaged over all trees of the ensemble.
    &lt;ul&gt;
      &lt;li&gt;The higher the Gini importance value, the more important the feature&lt;/li&gt;
      &lt;li&gt;Elbow method used to evaluate the number of features to use&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: RF&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/FP_RF.png&quot; alt=&quot;FP RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Faulty Plates dataset has a lot of important features, and has a sharp decline at 25 features.&lt;/li&gt;
  &lt;li&gt;Decrease in dimensionality from 27 to 24&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Faulty Steel Plates: RF Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/FP_RF_Clustering.png&quot; alt=&quot;FP RF Clustering&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/FP_RF_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/FP_RF_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 25 dimensions for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Doesn’t really have an elbow point&lt;/li&gt;
      &lt;li&gt;Doesn’t decrease within cluster error in k-means as much as the other algorithms&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Peak at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;k=7 for k-means&lt;/li&gt;
      &lt;li&gt;k=8 for EM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Accuracy shows that k=9 and k=4
Good clusters at k=4 and k=9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: RF&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/BC_RF.png&quot; alt=&quot;FP RF&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;10 important features, because Gini Importance starts to level off after that&lt;/li&gt;
  &lt;li&gt;Features reduced from 30 to 10 using Random Forest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Breast Cancer: RF Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;!-- &lt;img src=&quot;../images/ml-unsup/BC_RF_Clustering.png&quot; alt=&quot;FP RF Clustering&quot;&gt; --&gt;
&lt;p&gt;&lt;a href=&quot;../images/ml-unsup/BC_RF_Clustering.png&quot;&gt; &lt;img src=&quot;../images/ml-unsup/BC_RF_Clustering.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using 10 dimensions for clustering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;K-means: SSE
    &lt;ul&gt;
      &lt;li&gt;Elbow at k=7&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EM: Log-Likelihood
    &lt;ul&gt;
      &lt;li&gt;Leveling out around k=9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Silhouette Score
    &lt;ul&gt;
      &lt;li&gt;For both shows a consistent decrease&lt;/li&gt;
      &lt;li&gt;As the number of clusters get larger, the cluster distances for both start decreasing, hard to tell what a good k is&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Accuracy and Adjusted Mutual Information
    &lt;ul&gt;
      &lt;li&gt;Accuracy shows that k=3, 6, and 10 are good clusters
Good clusters at k=3, 6, and 10&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-3-dimensionality-reduction-algorithms-with-neural-network&quot;&gt;Part 3: Dimensionality Reduction Algorithms with Neural Network&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The faulty plates dataset is used to evaluate the dimensional reduction algorithms as inputs with the Neural Network compared to the baseline Neural Network&lt;/li&gt;
  &lt;li&gt;The baseline for the Neural Network is a 71.9% with hyperparameters of learning rate = 0.1, momentum = 0.3, and one hidden layer with 14 neurons.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/DR_NN.png&quot; alt=&quot;DR NN&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compares accuracy of different reduction algorithms on the Neural Network to the benchmark Neural Network&lt;/li&gt;
  &lt;li&gt;Accuracy shows that PCA and Random Projection perform well on the NN at higher components&lt;/li&gt;
  &lt;li&gt;With around 15 components, the accuracy is close to the benchmark&lt;/li&gt;
  &lt;li&gt;Dimensions are much lower than the original 27 features, so it simplifies model complexity&lt;/li&gt;
  &lt;li&gt;Compute times are similar between the benchmark, random projection, and PCA.
    &lt;h2 id=&quot;part-4-cluster-features-with-neural-network&quot;&gt;Part 4: Cluster Features with Neural Network&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;Clusters are used as features for the neural network&lt;/li&gt;
  &lt;li&gt;Baseline is the same as above&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/CF_NN.png&quot; alt=&quot;CF NN&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When clusters are used as features, it doesn’t do a good job of predicting the labels&lt;/li&gt;
  &lt;li&gt;Performs far below benchmark accuracy of 71.9%&lt;/li&gt;
  &lt;li&gt;Seems like accuracy is best for EM and k-means when k=6&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h2&gt;
&lt;p&gt;The dimensions for each of the components in the clustering algorithms in Part 2 along with computational times were as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ml-unsup/Conclusion.png&quot; alt=&quot;Conclusion&quot; width=&quot;350px&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Computational times were similar for PCA and ICA based on the dimensions&lt;/li&gt;
  &lt;li&gt;Random preojection did best in computational time and in lower dimensions&lt;/li&gt;
  &lt;li&gt;Random Forest used as feature selection takes too much computational time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the dimensionality reductions do help in reducing SSE within clusters. PCA and ICA do not increase the intra-cluster distance, but Random Projection and Random Forest does.&lt;/p&gt;</content><author><name>Justin Gong</name></author><category term="unsupervised learning" /><category term="python" /><category term="jython" /><summary type="html">Unsupervised Learning, Dimensionality Reduction, Clustering, PCA, ICA</summary></entry><entry><title type="html">M.O.R.E. Application</title><link href="http://localhost:4000/more/" rel="alternate" type="text/html" title="M.O.R.E. Application" /><published>2018-06-01T00:00:00-07:00</published><updated>2018-06-01T00:00:00-07:00</updated><id>http://localhost:4000/more</id><content type="html" xml:base="http://localhost:4000/more/">&lt;p&gt;Code for this project is available on &lt;a href=&quot;https://github.com/jjgong7/DVA-Project&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Deployed project on AWS Elastic Beanstalk is available &lt;a href=&quot;http://more-project.us-east-1.elasticbeanstalk.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;poster&quot;&gt;Poster:&lt;/h2&gt;
&lt;!-- &lt;img src=&quot;../images/more-app/poster.png&quot; alt=&quot;poster&quot; class=&quot;img&quot; style=&quot;cursor: zoom-in;&quot;&gt;   --&gt;
&lt;p&gt;&lt;a href=&quot;../images/more-app/poster.png&quot;&gt; &lt;img src=&quot;../images/more-app/poster.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;</content><author><name>Justin Gong</name></author><category term="python" /><category term="javascript" /><category term="SQL" /><category term="D3.js" /><summary type="html">Python, Javascript, SQL, D3.js</summary></entry><entry><title type="html">Computational Data Analytics Assignments</title><link href="http://localhost:4000/cda/" rel="alternate" type="text/html" title="Computational Data Analytics Assignments" /><published>2018-05-02T00:00:00-07:00</published><updated>2018-05-02T00:00:00-07:00</updated><id>http://localhost:4000/cda</id><content type="html" xml:base="http://localhost:4000/cda/">&lt;!-- 
# Computational Data Analysis --&gt;
&lt;p&gt;Code for this project is available on &lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;description&quot;&gt;Description:&lt;/h2&gt;
&lt;p&gt;Course includes hands-on introduction to programming techniques relevant to data analysis and machine learning. Most of the programming exercises are based on Python and SQL.&lt;/p&gt;

&lt;p&gt;Notebooks are built “from scratch,” of the basic components of a data analysis pipeline: collection, preprocessing, storage, analysis, and visualization. There are several examples of high-level data analysis questions, concepts and techniques for formalizing those questions into mathematical or computational tasks, and methods for translating those tasks into code. Beyond programming and best practices, notebooks include elementary data processing algorithms, notions of program correctness and efficiency, and numerical methods for linear
algebra and mathematical optimization.&lt;/p&gt;

&lt;h2 id=&quot;notebooks&quot;&gt;Notebooks:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%201&quot;&gt;Notebook 1: Python Essentials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%202&quot;&gt;Notebook 2: Pairwise Association Mining&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Notebook 3: Math Review (Not present)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%204&quot;&gt;Notebook 4: Representing Numbers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%205&quot;&gt;Notebook 5: Preprocessing Unstructured Text (Regex)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%206&quot;&gt;Notebook 6: Mining the Web (BeautifulSoup, APIs, JSON)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%207&quot;&gt;Notebook 7: Tidying Data (Tibbles, Melting, and Casting)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%208&quot;&gt;Notebook 8: Visualizing Data and Results (Bokeh, Seaborn)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%209&quot;&gt;Notebook 9: Relational Data (SQL, SQLite3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2010&quot;&gt;Notebook 10: Numerical Computing with Numpy/Scipy (Sparse Matrix, COO, CSR)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2011&quot;&gt;Notebook 11: Ranking Relational Objects (Markov Chain Analysis)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2012&quot;&gt;Notebook 12: Linear Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2013&quot;&gt;Notebook 13: Classification (Logistic Regression)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2014&quot;&gt;Notebook 14: Clustering via k-means&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2015&quot;&gt;Notebook 15: Compression via PCA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Computational-Data-Analytics/tree/master/Notebook%2016&quot;&gt;Notebook 16: Eigenfaces&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt;&lt;br /&gt;
Python 3.6, SQL&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Libraries:&lt;/strong&gt;&lt;br /&gt;
Pandas, NumPy, SciPy, re, matplotlib, seaborn, bokeh, collections, itertools&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relational Database Management System:&lt;/strong&gt;&lt;br /&gt;
SQLite3&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Environments:&lt;/strong&gt;&lt;br /&gt;
Jupyter Notebooks&lt;/p&gt;</content><author><name>Justin Gong</name></author><category term="machine learning" /><category term="python" /><category term="supervised learning" /><summary type="html">Python, Numpy, Pandas, Supervised Learning, Data Science, Machine Learning</summary></entry><entry><title type="html">Randomized Optimization</title><link href="http://localhost:4000/ro/" rel="alternate" type="text/html" title="Randomized Optimization" /><published>2018-05-01T00:00:00-07:00</published><updated>2018-05-01T00:00:00-07:00</updated><id>http://localhost:4000/ro</id><content type="html" xml:base="http://localhost:4000/ro/">&lt;p&gt;Details for this project are available on &lt;a href=&quot;https://github.com/jjgong7/Machine-Learning/tree/master/2%20-%20Randomized%20Optimization&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;report&quot;&gt;Report:&lt;/h2&gt;
&lt;object data=&quot;https://github.com/jjgong7/Machine-Learning/raw/master/2%20-%20Randomized%20Optimization/Analysis.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;
&lt;/object&gt;</content><author><name>Justin Gong</name></author><category term="randomized optimization" /><category term="jython" /><category term="java" /><summary type="html">Randomized Optimization, ABAGAIL, Jython, Java, Jupyter Notebooks, Python</summary></entry><entry><title type="html">Markov Decision Processes</title><link href="http://localhost:4000/mdp/" rel="alternate" type="text/html" title="Markov Decision Processes" /><published>2018-04-02T00:00:00-07:00</published><updated>2018-04-02T00:00:00-07:00</updated><id>http://localhost:4000/mdp</id><content type="html" xml:base="http://localhost:4000/mdp/">&lt;p&gt;Details for this project are available on &lt;a href=&quot;https://github.com/jjgong7/Machine-Learning/tree/master/4%20-%20Markov%20Decision%20Processes&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;report&quot;&gt;Report:&lt;/h2&gt;
&lt;object data=&quot;https://github.com/jjgong7/Machine-Learning/raw/master/4%20-%20Markov%20Decision%20Processes/Analysis.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;
&lt;/object&gt;</content><author><name>Justin Gong</name></author><category term="markov decision processes" /><category term="jython" /><category term="java" /><summary type="html">Markov Decision Processes, BURLAP, Jython, Java, Jupyter Notebooks, Python</summary></entry><entry><title type="html">Space Shuttle Challenger</title><link href="http://localhost:4000/bay/" rel="alternate" type="text/html" title="Space Shuttle Challenger" /><published>2018-04-01T00:00:00-07:00</published><updated>2018-04-01T00:00:00-07:00</updated><id>http://localhost:4000/bay</id><content type="html" xml:base="http://localhost:4000/bay/">&lt;p&gt;Code for this project is available on &lt;a href=&quot;https://github.com/jjgong7/Bayesian-Statistics&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction:&lt;/h2&gt;

&lt;p&gt;On January 28, 1986, space shuttle Challenger was due to takeoff from Kennedy Space Center on its tenth space mission to deploy satellites and observe Halley’s Comet among other things.&lt;/p&gt;

&lt;p&gt;Space shuttle Challenger contained two Solid Rocket Boosters (SRBs) that boosts the shuttle into space. Each booster has three field joints, which contains two O-rings (secondary and primary). These O-rings are 37 foot circles of special rubber that help pressure seal the gas and exhaust.&lt;/p&gt;

&lt;p&gt;It was an unusually cold morning that day, measuring 31 degrees Fahrenheit. Ice was visible around the launchpad and on the equipment. NASA had no experience launching at such cold temperatures with the coldest previous launch being 22 degrees warmer, at 53 degrees Fahrenheit. There was concern about the durability of the O-rings because they haven’t been tested at such low temperatures and could fail by erosion or blowby. However, there wasn’t enough support to abort and they decided to continue on with the mission.&lt;/p&gt;

&lt;p&gt;73 seconds after liftoff, at an altitude of 9 miles, the O-ring on one of the boosters malfunctioned. The rubber O-rings had become stiff under the cold and failed to seal the joint, letting a plume of exhaust to leak out. Hot gases enveloped the cold hull of an external tank full of gases and ruptured it, causing the shuttle to be torn apart by aerodynamic forces.&lt;/p&gt;

&lt;p&gt;The space shuttle had no escape hatch, so it went into a free fall and disintegrated going terminal velocity over the Atlantic Ocean near Cape Canaveral, Florida. The outcome of this terrible tragedy resulted in the death of 7 crewmembers and a nearly 3 year hiatus of the space program.&lt;/p&gt;

&lt;p&gt;This tragedy could have possibly been prevented with enough statistical analysis using Bayesian Linear Regression by predicting the Number of O-rings experiencing thermal distress given historical data from 23 shuttle flights previous to the Challenger disaster.&lt;/p&gt;

&lt;h2 id=&quot;description-of-dataset&quot;&gt;Description of Dataset:&lt;/h2&gt;
&lt;p&gt;The dataset is found on the UCI Machine Learning Repository. The donor of the data is David Draper of UCLA.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are 23 instances with 4 attributes and a response variable.&lt;/li&gt;
  &lt;li&gt;The four attributes are:
    &lt;ol&gt;
      &lt;li&gt;The number of O-rings at risk on a given flight&lt;/li&gt;
      &lt;li&gt;Launch temperature (degrees Fahrenheit)&lt;/li&gt;
      &lt;li&gt;Leak-check pressure (psi)&lt;/li&gt;
      &lt;li&gt;Temporal order of flight.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Number of O-rings is always 6 and the temporal order of flight isn’t necessary since the flights are independent of each other. The data for Number of O-rings at risk on a given flight and Temporal order of flight is still listed in the dataset in OpenBUGS, but not used for the linear regression models.&lt;/p&gt;

&lt;p&gt;As a result, there are really only two relevant attributes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Launch Temperature (degrees F)&lt;/li&gt;
  &lt;li&gt;Leak-Check pressure
    &lt;ul&gt;
      &lt;li&gt;The pressure (in psi) at which safety testing for field joint leaks was performed, was available, but the relevance to the problem was unclear.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, I will run two linear regression models:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;One with 3 parameters – intercept, Launch Temperature (degrees F) and Leak-Check pressure and&lt;/li&gt;
  &lt;li&gt;One with 2 parameters – intercept and Launch Temperature (degrees F).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The goal is to make a prediction of how many O-rings will experience thermal distress at 31 degrees Fahrenheit with Leak-Check pressure of 50, 100, and 200, and with temperature alone in the second model.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-linear-regression-vs-ols-linear-regression&quot;&gt;Bayesian Linear Regression vs OLS Linear Regression:&lt;/h2&gt;
&lt;p&gt;The Bayesian approach compared to OLS is that instead of maximizing the likelihood function alone, we assume prior distributions for the parameters and use Bayes theorem. Since I don’t have any estimates for the prior parameters, I will be setting normal non-informative priors on the beta coefficients and a gamma prior on the precision parameter.&lt;/p&gt;

&lt;p&gt;By applying Bayes theorem, we obtain the posterior distribution of the parameters, which is an estimate that depends on both the data’s likelihood function and the prior belief. Bayesian Linear Regression doesn’t find the single “best” value of the model parameters, but determines the posterior distribution for the model parameters.&lt;/p&gt;

&lt;p&gt;Depending on priors, Bayesian analysis is generally more accurate in small samples such as this problem with only 23 instances. With more information in the data, the priors are less influential.&lt;/p&gt;

&lt;h2 id=&quot;summary-of-data&quot;&gt;Summary of Data:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;../images/bay/summary.png&quot;&gt; &lt;img src=&quot;../images/bay/summary.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Summary of the data shows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The mean launch temp is 69.57 degrees Fahrenheit, which is much warmer than the temperature on that day.&lt;/li&gt;
  &lt;li&gt;Leak pressure is 152.2 psi on average, but it takes values of 50, 100, and 200.&lt;/li&gt;
  &lt;li&gt;There are 7 occurrences of O-rings experiencing thermal distress, with 5 involving 1 O-ring and 2 involving 2 O-rings.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis:&lt;/h2&gt;
&lt;p&gt;Both are sampled at 1,000,000 iterations and 1,000 burned.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;With 3 parameters: Intercept, Launch Temperature (degrees F) and Leak-Check pressure&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;../images/bay/3_param.png&quot;&gt; &lt;img src=&quot;../images/bay/3_param.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Beta[1] is intercept, beta[2] is Launch Temperature, and beta[3] is Leak-Check Pressure. From the 95% Credible Set, I can see that the intercept and launch temperature coefficients are statistically significant because the set doesn’t contain 0. Leak-Check pressure has a 95% Credible Set of (-6.623E-4, 0.00656) and barely includes 0 in the lower bound. It is possible that this coefficient isn’t significant. This Leak Pressure will be taken out in the 2 parameter model.&lt;/p&gt;

&lt;p&gt;The mean response of the number of O-rings experiencing thermal distress when Launch temperature = 31 degrees Fahrenheit and Leak-check pressure (psi) = 50, 100, and 200 are as follows:&lt;/p&gt;

&lt;p&gt;Leak-check pressure (psi) = 50, Thermal distress = 1.96, with 95% Credible Set (0.5725, 3.35)&lt;/p&gt;

&lt;p&gt;Leak-check pressure (psi) = 100, Thermal distress = 2.11, with 95% Credible Set (0.7468, 3.466)&lt;/p&gt;

&lt;p&gt;Leak-check pressure (psi) = 200, Thermal distress = 2.40, with 95% Credible Set (1.027, 3.766)&lt;/p&gt;

&lt;p&gt;The 95% Credible Sets for prediction (new.TD_50, 100, 200) are much larger than the mean response, as expected, because there is more error in prediction.&lt;/p&gt;

&lt;p&gt;Since the mean for coefficient beta[3] is 0.002943, it means that each unit of Leak-check pressure (psi) increases the number of O-rings experiencing thermal distress by 0.002943. From 50 psi to 200 psi, there is almost a .5 increase in O-rings experiencing thermal distress. A higher leak-check pressure is not better. This is a little strange and contrary to what I would believe - that checking leaks at a higher pressures should mean it’s more safe. However, this could also mean that they are expecting the rocket booster to withstand more pressure based on cargo or whatever and believe it’s necessary to test at higher pressures.&lt;/p&gt;

&lt;p&gt;The R-squared = 0.2829 and the adjusted R-squared = 0.2112. The R-squared is a goodness-of-fit measure and explains the variability in the data. It’s small at .2 something. A higher R-squared is better and desired, but not possible with the given data. Adjusted R-squared modifies for the number of predictors in the model. The small R-squared could make the results more skeptical. The deviance is 39.19.&lt;/p&gt;

&lt;p&gt;Still, it’s quite clear that regardless of the leak-check pressure, due to a very lower than normal temperature at launch of 31 degrees Fahrenheit, the chance of O-ring malfunction is very high and that the mission should have been aborted. The mean response of the number of O-rings experiencing thermal distress is around 2 for the three different values of leak-check pressure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;With 2 parameters: Intercept and Launch Temperature (degrees F)&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;../images/bay/2_param.png&quot;&gt; &lt;img src=&quot;../images/bay/2_param.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Beta[1] is intercept and beta[2] is Launch Temperature. From the 95% Credible Set, I can see that the intercept and launch temperature coefficients are statistically significant because the set doesn’t contain 0.&lt;/p&gt;

&lt;p&gt;The mean response of the number of O-rings experiencing thermal distress when Launch temperature = 31 degrees Fahrenheit is as follows:&lt;/p&gt;

&lt;p&gt;Thermal distress = 2.245, with 95% Credible Set (0.8436, 3.668)&lt;/p&gt;

&lt;p&gt;The 95% Credible Sets for prediction (new.TD) is much larger than the mean response as expected because there is more error in prediction.&lt;/p&gt;

&lt;p&gt;The R-squared = 0.184 and the adjusted R-squared = 0.1452. The R-squared is a goodness-of-fit measure and explains the variability in the data. The R-squared values are much worse without Leak-check pressure as a variable. The R-squared is 0.184 vs 0.2829 and R-squared adjusted is 0.1452 vs. 0.2112. The deviance is also higher at 41.11 vs 39.19 and variance (sigma squared) is higher as well.&lt;/p&gt;

&lt;p&gt;Due to a very lower than normal temperature at launch of 31 degrees Fahrenheit, the chance of O-ring malfunction is very high and that the mission should be aborted.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h2&gt;
&lt;p&gt;In comparing the two models, the 3 parameter model with the intercept, launch temperature and leak-check pressure performs better than the 2 parameter model. The 3 parameter model explains much more of the variability in the data with the higher R-squared and R-squared adjusted value and has a slightly lower deviance. Having the leak-check pressure coefficient also explains a 0.5 variability in the number of O-rings experiencing thermal distress from 50 to 200 psi. With both models, it’s clear that with a launch temperature of 31 degrees Fahrenheit, the number of O-rings experiencing thermal distress is very high at around 2. If they performed regression using data from the 23 previous flights, they could have concluded there was a high chance of malfunction predicted and that the mission should not have been flown.&lt;/p&gt;</content><author><name>Justin Gong</name></author><category term="bayesian statistics" /><category term="WinBUGS" /><summary type="html">Bayesian Statistics, WinBugs</summary></entry><entry><title type="html">Database Systems - Emergency Resource Management System</title><link href="http://localhost:4000/dbm/" rel="alternate" type="text/html" title="Database Systems - Emergency Resource Management System" /><published>2018-03-02T00:00:00-08:00</published><updated>2018-03-02T00:00:00-08:00</updated><id>http://localhost:4000/dbm</id><content type="html" xml:base="http://localhost:4000/dbm/">&lt;p&gt;Code for this project is available on &lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;database-systems---team-project&quot;&gt;Database Systems - Team Project&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Team Members:&lt;/strong&gt;&lt;br /&gt;
Evan Althouse, Chang-Zhou Gong, Tich Mangono, David Ribeiro&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt;&lt;br /&gt;
Python, SQL, Javascript, HTML, CSS&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Libraries:&lt;/strong&gt;&lt;br /&gt;
jQuery&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Frameworks:&lt;/strong&gt;&lt;br /&gt;
Flask, Bootstrap&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relational Database Management System:&lt;/strong&gt;&lt;br /&gt;
MariaDB (MySQL)&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview:&lt;/h2&gt;

&lt;h3 id=&quot;emergency-resource-management-system-erms&quot;&gt;Emergency Resource Management System (ERMS)&lt;/h3&gt;
&lt;p&gt;Build an information management tool that supports government agencies and municipalities in locating and activating resources after an emergency such as a natural disaster, hazardous material spill, etc. The system is used because such events require resources above and beyond the set of resources that would normally meet a municipality’s typical operational needs. Users of the system can:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Add a Resource&lt;/strong&gt; Allows the user to add resources that will be available for use in case of a nearby emergency incident. Other users in the system will be able to search for and request these resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Add an Incident&lt;/strong&gt; The user selects this option in order to add some basic information about an emergency incident that has just occurred.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Search for Resources&lt;/strong&gt; This option allows the user to search for and request available resources in the case of an emergency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Status&lt;/strong&gt; This option allows the user to view currently deployed resources and manage resource requests that she has sent or received.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Report&lt;/strong&gt; This option shows a summary report of all the user’s resources grouped by their primary Emergency Support Function.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;project-separated-into-three-phases&quot;&gt;Project Separated into Three Phases:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Phase 1&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%201/team05_p1_eer.pdf&quot;&gt;team05_p1_eer.pdf&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Enhanced Entity-Relationship (EER) Diagram&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%201/team05_p1_ifd.pdf&quot;&gt;team05_p1_ifd.pdf&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Information Flow Diagram (IFD)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%201/team05_p1_report.pdf&quot;&gt;team05_p1_report.pdf&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Report with Data Types, Business Logic Constraints, Task Decomposition (TD) &amp;amp; Abstract Code (AC)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phase 2&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%202/team05_p2_ac+SQL.pdf&quot;&gt;team05_p2_ac+SQL.pdf&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Abstract Code with in-line SQL queries added&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%202/team05_p2_eer2rel.pdf&quot;&gt;team05_p2_eer2rel.pdf&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Enhanced Entity-Relationship (EER) to Relational Mapping&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%202/team05_p2_updatedEER.pdf&quot;&gt;team05_p2_updatedEER.pdf&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Updated Enhanced Entity-Relationship (EER)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/blob/master/Phase%202/team05_p2_schema.sql&quot;&gt;team05_p2_schema.sql&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;SQL Create Table statements to create schema (tables with constraints and keys)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phase 3&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Implementation/Code&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;phase-3&quot;&gt;Phase 3&lt;/h1&gt;

&lt;h2 id=&quot;instructions-to-run&quot;&gt;Instructions to Run:&lt;/h2&gt;

&lt;h3 id=&quot;setup-database-locally&quot;&gt;Setup Database Locally:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Using Ubuntu or Mac:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Change to root (sudo) user&lt;/li&gt;
  &lt;li&gt;cd into the directory of this file (setup folder)&lt;/li&gt;
  &lt;li&gt;Set file permissions:&lt;br /&gt;
     &lt;code class=&quot;highlighter-rouge&quot;&gt;bash
     chmod a+rwx &amp;lt;setupUbuntu.sh or setupMac.sh&amp;gt;
    &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Run setup shell:
    &lt;ul&gt;
      &lt;li&gt;For Ubuntu:
        &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt; | ./setupUbuntu.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;For Mac:
        &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt; | ./setupMac.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If permissions denied, input the root password in lines 25 to 28 –password= pw and uncomment. Comment out lines 21 to 24.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;running-app&quot;&gt;Running App:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;cd into directory with app.py&lt;/li&gt;
  &lt;li&gt;In terminal run:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python app.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Connect to local host (e.g. http://127.0.0.1:5000/) with browser&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pages&quot;&gt;Pages&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Overview:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Add a Resource&lt;/strong&gt; Allows the user to add resources that will be available for use in case of a nearby emergency incident. Other users in the system will be able to search for and request these resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Add an Incident&lt;/strong&gt; The user selects this option in order to add some basic information about an emergency incident that has just occurred.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Search for Resources&lt;/strong&gt; This option allows the user to search for and request available resources in the case of an emergency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Status&lt;/strong&gt; This option allows the user to view currently deployed resources and manage resource requests that she has sent or received.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Report&lt;/strong&gt; This option shows a summary report of all the user’s resources grouped by their primary Emergency Support Function.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;login&quot;&gt;Login&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/1.Login.jpg&quot; alt=&quot;Login&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-menu&quot;&gt;Main Menu&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/2.MainMenu.jpg&quot; alt=&quot;Main Menu&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;add-resource&quot;&gt;Add Resource&lt;/h2&gt;
&lt;h3 id=&quot;1-empty-form&quot;&gt;1. Empty Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/3a.AddResource.png&quot; alt=&quot;AddResource1&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-empty-form-submit-with-required-fields&quot;&gt;2. Empty Form Submit with Required Fields&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/3b.AddResourceReqField.png&quot; alt=&quot;AddResource2&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;3-filled-form&quot;&gt;3. Filled Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/3c.AddResourceFilled.png&quot; alt=&quot;AddResource3&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;4-submitted-form&quot;&gt;4. Submitted Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/3d.AddResourceSubmitted.png&quot; alt=&quot;AddResource4&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;add-incident&quot;&gt;Add Incident&lt;/h2&gt;
&lt;h3 id=&quot;1-empty-form-1&quot;&gt;1. Empty Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/4a.AddIncident.png&quot; alt=&quot;AddIncident1&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-empty-form-submit-with-required-fields-1&quot;&gt;2. Empty Form Submit with Required Fields&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/4b.AddIncidentReqField.png&quot; alt=&quot;AddIncident2&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;3-filled-form-1&quot;&gt;3. Filled Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/4c.AddIncidentFilled.png&quot; alt=&quot;AddIncident3&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;4-submitted-form-1&quot;&gt;4. Submitted Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/4d.AddIncidentSubmitted.png&quot; alt=&quot;AddIncident4&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;search-resources&quot;&gt;Search Resources&lt;/h2&gt;
&lt;h3 id=&quot;1-empty-form-2&quot;&gt;1. Empty Form&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5a.SearchResources.png&quot; alt=&quot;SearchResources1&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-empty-form-search---returns-all-incidents&quot;&gt;2. Empty Form Search - Returns All Incidents&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5b.SearchResourcesAllIncidents.png&quot; alt=&quot;SearchResources2&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;3a-search-ford&quot;&gt;3a. Search Ford&lt;/h3&gt;
&lt;h4 id=&quot;search-criteria---keyword-ford-esf-transportation-location-500-incident-md-3-earthquake&quot;&gt;Search Criteria - Keyword: Ford; ESF: Transportation; Location: 500; Incident: MD-3: Earthquake&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5c.SearchResourcesFilled.png&quot; alt=&quot;SearchResources3&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;3b-search-ford---results&quot;&gt;3b. Search Ford - Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5d.SearchResults.png&quot; alt=&quot;SearchResources4&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;4a-search-incident-md-2-volcano&quot;&gt;4a. Search Incident: MD-2: Volcano&lt;/h3&gt;
&lt;h4 id=&quot;search-criteria---esf-transportation-location-5000-incident-md-2-volcano&quot;&gt;Search Criteria - ESF: Transportation; Location: 5000; Incident: MD-2: Volcano&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5e.SearchVolcano.png&quot; alt=&quot;SearchResources5&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;4b-search-ford---results&quot;&gt;4b. Search Ford - Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5e.SearchResultsVolcano.png&quot; alt=&quot;SearchResources6&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;4c-search-ford---requested-fbi-police-car-pending-request&quot;&gt;4c. Search Ford - Requested FBI Police Car (Pending Request)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/5e.SearchResultsVolcanoReq.png&quot; alt=&quot;SearchResources7&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;resource-status&quot;&gt;Resource Status&lt;/h2&gt;
&lt;h3 id=&quot;1-resource-status-for-current-user-testindividual&quot;&gt;1. Resource Status for Current User (testindividual)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/6a.ResourceStatus.png&quot; alt=&quot;ResourceStatus1&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-return-resource-other-users-resource---peters-other-fire-truck&quot;&gt;2. Return Resource (Other User’s Resource) - Peter’s Other Fire Truck&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/6b.ResourceStatusReturn.png&quot; alt=&quot;ResourceStatus2&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;3-deploy-resource-owned-resource---fire-truck&quot;&gt;3. Deploy Resource (Owned Resource) - Fire Truck&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/6c.ResourceRequestDeploy.png&quot; alt=&quot;ResourceStatus3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;resource-report&quot;&gt;Resource Report&lt;/h2&gt;
&lt;h3 id=&quot;resource-report-by-primary-esf&quot;&gt;Resource Report By Primary ESF&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jjgong7/Database-Systems-Concepts-and-Design/raw/master/Phase%203/images/7a.ResourceReport.png&quot; alt=&quot;ResourceReport1&quot; /&gt;&lt;/p&gt;</content><author><name>Justin Gong</name></author><category term="python" /><category term="flask" /><category term="sql" /><summary type="html">Python, Javascript, Database Systems, MySQL, Flask</summary></entry><entry><title type="html">Data and Visual Analytics</title><link href="http://localhost:4000/dva/" rel="alternate" type="text/html" title="Data and Visual Analytics" /><published>2018-03-01T00:00:00-08:00</published><updated>2018-03-01T00:00:00-08:00</updated><id>http://localhost:4000/dva</id><content type="html" xml:base="http://localhost:4000/dva/">&lt;h2 id=&quot;description&quot;&gt;Description:&lt;/h2&gt;
&lt;p&gt;Four assignments demonstrating techniques and tools for analyzing and visualizing data at scale. Languages &amp;amp; Frameworks used: JavaScript (D3.js), Python, Java, Scala, Pig, SQL, HTML, CSS, Hadoop, Spark, and AWS.&lt;/p&gt;

&lt;h2 id=&quot;data-and-visual-analytics-assignments&quot;&gt;Data and Visual Analytics Assignments:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Data-and-Visual-Analytics/tree/master/HW1-Python%2C%20SQLite%2C%20D3%2C%20Gephi%2C%20and%20OpenRefine&quot;&gt;HW1-Python, SQLite, D3, Gephi, and OpenRefine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Data-and-Visual-Analytics/tree/master/HW2-D3%20Graphs%20and%20Visualization&quot;&gt;HW2-D3 Graphs and Visualization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Data-and-Visual-Analytics/tree/master/HW3-Hadoop%2C%20Spark%2C%20Pig%2C%20and%20Azure&quot;&gt;HW3-Hadoop, Spark, Pig, AWS and Azure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jjgong7/Data-and-Visual-Analytics/tree/master/HW4-Scalable%20PageRank%2C%20Random%20Forest%2C%20Weka&quot;&gt;HW4-Scalable PageRank, Random Forest, Weka&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt;&lt;br /&gt;
Javascript, Python (2.7), Java, Scala, Pig, SQL, HTML, CSS&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Libraries:&lt;/strong&gt;&lt;br /&gt;
D3.js, TopoJSON, NumPy, SciPy&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Frameworks:&lt;/strong&gt;&lt;br /&gt;
Flask, Hadoop, Spark&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relational Database Management System:&lt;/strong&gt;&lt;br /&gt;
SQLite3&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Software/Applications:&lt;/strong&gt;&lt;br /&gt;
Weka, Gephi, OpenRefine, Tableau, VirtualBox (with VM image from CDH), Azure ML Studio&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cloud Services:&lt;/strong&gt; &lt;br /&gt;
Amazon Web Services (AWS):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Storage (S3)&lt;/li&gt;
  &lt;li&gt;Elastic Cloud Computing (EC2)&lt;/li&gt;
  &lt;li&gt;Elastic MapReduce (EMR)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Microsoft Azure:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Azure Blob Storage&lt;/li&gt;
  &lt;li&gt;HDInsight (Linux Cluster)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justin Gong</name></author><category term="python" /><category term="javascript" /><category term="SQL" /><category term="D3.js" /><category term="AWS" /><category term="Big Data" /><summary type="html">JavaScript (D3.js), Python, Java, Scala, Pig, SQL, HTML, CSS, Hadoop, Spark, AWS</summary></entry></feed>